# exporter/exporter.py

"""Handles preparation and formatting of rule data for export."""

import logging
import collections
import json
import os
import asyncio
import inspect
import shutil
import datetime  # Added import
from automate_frontend_tests import run_frontend_tests
from typing import Any, Dict, List, Set, Optional
from collections import defaultdict
import ast

import Utils  # Added import
from .analyzer import analyze_rule
from .games import get_game_export_handler

logger = logging.getLogger(__name__)

def is_serializable(obj):
    """Check if an object can be serialized to JSON."""
    try:
        json.dumps(obj)
        return True
    except (TypeError, OverflowError, ValueError):
        return False

def make_serializable(obj):
    """
    Recursively convert an object to be JSON serializable.
    Extracts values from enums and custom objects intelligently.
    """
    # Handle basic types directly
    if obj is None or isinstance(obj, (bool, int, float, str)):
        return obj
    
    # Handle dictionaries
    if isinstance(obj, dict):
        serialized_dict = {str(k): make_serializable(v) for k, v in obj.items()}
        
        return serialized_dict
    
    # Handle lists, tuples, and sets
    if isinstance(obj, (list, tuple, set)):
        return [make_serializable(i) for i in obj]
    
    # Handle objects with __dict__ attribute (custom classes)
    if hasattr(obj, '__dict__'):
        # First check for value attribute (common in enums)
        if hasattr(obj, 'value'):
            return make_serializable(obj.value)
        
        # Try to extract value from string representation like "Type(Value)"
        str_rep = str(obj)
        if '(' in str_rep and ')' in str_rep:
            try:
                # Extract value inside parentheses
                extracted = str_rep.split('(', 1)[1].split(')', 1)[0]
                
                # Try to convert to appropriate type
                if extracted.lower() in ('yes', 'no', 'true', 'false'):
                    return extracted.lower() == 'yes' or extracted.lower() == 'true'
                elif extracted.isdigit():
                    return int(extracted)
                else:
                    return extracted
            except Exception as e:
                # If extraction fails, log and use string representation
                return str_rep
        
        # If no special handling applies, use string representation
        return str_rep
    
    # If all else fails, convert to string
    if not is_serializable(obj):
        return str(obj)
    
    return obj

def debug_export_data(data, prefix=""):
    """
    Recursively checks each field of the export data to find non-serializable parts.
    Logs detailed information about which fields might be causing issues.
    """
    if isinstance(data, dict):
        for key, value in data.items():
            current_path = f"{prefix}.{key}" if prefix else str(key)
            try:
                # Try to serialize just this item
                json.dumps({key: value})
                # If it succeeded, go deeper
                debug_export_data(value, current_path)
            except (TypeError, OverflowError, ValueError) as e:
                logger.error(f"Non-serializable field at {current_path}: {str(e)}")
                # Try to diagnose deeper
                debug_export_data(value, current_path)
    elif isinstance(data, (list, tuple)):
        for i, item in enumerate(data):
            current_path = f"{prefix}[{i}]"
            try:
                json.dumps(item)
                debug_export_data(item, current_path)
            except (TypeError, OverflowError, ValueError) as e:
                logger.error(f"Non-serializable item at {current_path}: {str(e)}")
                debug_export_data(item, current_path)
    else:
        # For leaf nodes, just check if they're serializable
        try:
            json.dumps(data)
        except (TypeError, OverflowError, ValueError) as e:
            logger.error(f"Non-serializable value at {prefix}: {data} (type: {type(data)}) - {str(e)}")

def write_field_by_field(export_data, filepath):
    """
    Tries to write each major section of the export_data to the file separately,
    to ensure at least some data is saved even if one section is problematic.
    """
    serializable_data = {"version": export_data.get("version", 1)}
    fields_written = []
    
    # Try each field separately
    for field in ["regions", "items", "item_groups", "progression_mapping", "settings", "start_regions", "game_info", "itempool_counts"]:
        if field in export_data:
            try:
                serializable_field = make_serializable(export_data[field])
                # Test if it's serializable
                json.dumps(serializable_field)
                serializable_data[field] = serializable_field
                fields_written.append(field)
                logger.info(f"Successfully processed field: {field}")
            except Exception as e:
                error_msg = f"Failed to process field {field}: {str(e)}"
                logger.error(error_msg)
                
                # For complex fields, try to process each player separately
                if field in ["settings", "game_info"] and isinstance(export_data.get(field, {}), dict):
                    # Initialize with empty dict
                    serializable_data[field] = {}
                    
                    # Try each player separately
                    for player_id in export_data.get(field, {}):
                        try:
                            player_data = make_serializable(export_data[field][player_id])
                            json.dumps(player_data)  # Test serialization
                            serializable_data[field][player_id] = player_data
                            logger.info(f"Added {field} data for player {player_id}")
                        except Exception as player_error:
                            error_msg = f"Failed to process {field} for player {player_id}: {str(player_error)}"
                            logger.error(error_msg)
                            # Use error message instead of default
                            serializable_data[field][player_id] = f"ERROR: {error_msg}"
    
    # Write what we have
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(serializable_data, f, indent=2)
        logger.info(f"Wrote partial data ({', '.join(fields_written)}) to {filepath}")
        return True
    except Exception as e:
        error_msg = f"Failed to write even partial data: {str(e)}"
        logger.error(error_msg)
        return False

def prepare_export_data(multiworld) -> Dict[str, Any]:
    """
    Prepares complete game data for export to JSON format.
    Preserves as much of the Python backend's structure as possible.
    """
    export_data = {
        "schema_version": 3,  # Schema version for the export format
        "archipelago_version": Utils.__version__,
        "generation_seed": multiworld.seed,
        "player_names": multiworld.player_name, # Player ID -> Name mapping
        "plando_options": [option.name for option in multiworld.plando_options], # Active plando options
        "world_classes": {player: multiworld.worlds[player].__class__.__name__ 
                           for player in multiworld.player_ids}, # Player ID -> World Class Name mapping
        'regions': {},  # Full region graph
        'items': {},    # Item data by player
        'item_groups': {},  # Item groups by player
        'progression_mapping': {},  # Progressive item info
        'settings': {}, # Game settings by player
        'start_regions': {},  # Start regions by player
        'itempool_counts': {},  # NEW: Complete itempool counts by player
        'game_info': {},  # NEW: Game-specific information for frontend
    }

    for player in multiworld.player_ids:
        player_str = str(player) # Use player_str consistently
        
        # Get game name, world, and handler
        game_name = multiworld.game[player]
        world = multiworld.worlds[player]
        game_handler = get_game_export_handler(game_name)
        
        # Process all regions and their connections
        export_data['regions'][player_str] = process_regions(multiworld, player)
        
        # Process items and groups
        export_data['items'][player_str] = process_items(multiworld, player)
        export_data['item_groups'][player_str] = process_item_groups(multiworld, player)
        export_data['progression_mapping'][player_str] = process_progression_mapping(multiworld, player)

        # NEW: Get game-specific information if available using handler
        try:
            game_info = game_handler.get_game_info(world) # Use handler directly
            export_data['game_info'][player_str] = game_info
        except Exception as e:
            error_msg = f"Error getting game_info from handler for player {player}: {str(e)}"
            logger.error(error_msg)
            # Fallback to default
            export_data['game_info'][player_str] = {
                "name": game_name,
                "rule_format": {
                    "version": "1.0"
                }
            }

        # NEW: Process complete itempool data using handler
        try:
            export_data['itempool_counts'][player_str] = game_handler.get_itempool_counts(world, multiworld, player) # Call the handler method
            logger.debug(f"Successfully exported itempool counts via handler for player {player}")
        except Exception as e:
            error_msg = f"Error exporting itempool counts for player {player}: {str(e)}"
            logger.error(error_msg)
            export_data['itempool_counts'][player_str] = {
                'error': error_msg,
                'details': "Failed to read itempool counts. Check logs for more information."
            }

        # Get Settings using handler
        try:
            settings_data = game_handler.get_settings_data(world, multiworld, player) # Call the handler method
            export_data['settings'][player_str] = settings_data
        except Exception as e:
            error_msg = f"Error exporting settings for player {player}: {str(e)}"
            logger.error(error_msg)
            export_data['settings'][player_str] = {
                'error': error_msg,
                'details': "Failed to read game settings. Check logs for more information."
            }

        # Start regions
        try:
            logger.debug(f"Processing start regions for player {player}")
            # world variable should already be defined from earlier in the loop
            logger.debug(f"Using world object for player {player}")

            try:
                menu_region = multiworld.get_region('Menu', player)
                logger.debug(f"Got menu region for player {player}")
            except Exception as e:
                logger.error(f"Error getting menu region: {str(e)}")
                menu_region = None

            available_regions = []
            player_regions = [
                region for region in multiworld.get_regions() 
                if region.player == player
            ]
            logger.debug(f"Found {len(player_regions)} regions for player {player}")

            for region in player_regions:
                try:
                    if hasattr(region, 'can_start_at') and callable(getattr(region, 'can_start_at')):
                        logger.debug(f"Checking if can start at region: {region.name}")
                        try:
                            # Ensure world is passed to can_start_at if needed by the method
                            can_start = region.can_start_at(world) 
                            if (can_start):
                                region_data = {
                                    'name': region.name,
                                    'type': getattr(region, 'type', 'Region'), # Assuming extract_type_value is not needed here or applied later
                                    'dungeon': getattr(region.dungeon, 'name', None) if hasattr(region, 'dungeon') and region.dungeon else None,
                                    'is_light_world': getattr(region, 'is_light_world', False),
                                    'is_dark_world': getattr(region, 'is_dark_world', False)
                                }
                                available_regions.append(region_data)
                        except Exception as e:
                            logger.error(f"Error checking can_start_at for region {region.name}: {str(e)}")
                    else:
                        logger.debug(f"Region {region.name} does not have a callable can_start_at method.")
                except Exception as e:
                    logger.error(f"Error processing region {getattr(region, 'name', 'Unknown')} in start regions loop: {str(e)}")
                    continue

            export_data['start_regions'][player_str] = {
                'default': ['Menu'], # Keep default as Menu
                'available': available_regions
            }
            logger.debug(f"Completed processing start regions for player {player}")

        except Exception as e:
            logger.error(f"Error in top-level start regions processing for player {player}: {str(e)}")
            logger.exception("Full traceback:")
            # Provide a fallback in case of error
            export_data['start_regions'][player_str] = {
                'default': ['Menu'],
                'available': []
            }

    return export_data

def process_regions(multiworld, player: int) -> Dict[str, Any]:
    """
    Process complete region data including all available backend data.
    """
    logger.debug(f"Starting process_regions for player {player}")
    
    def safe_expand_rule(game_handler, rule_func,
                         rule_target_name: Optional[str] = None,
                         target_type: Optional[str] = None):
        """Analyzes rule using runtime analysis (analyze_rule)."""
        try:
            if not rule_func:
                return None

            logger.debug(f"safe_expand_rule: Analyzing {target_type} '{rule_target_name or 'unknown'}' using runtime analyze_rule")
            # Directly call analyze_rule, which handles recursion internally for combined rules
            analysis_result = analyze_rule(rule_func=rule_func)
            
            if analysis_result and analysis_result.get('type') != 'error':
                logger.debug(f"safe_expand_rule: Runtime analysis successful for '{rule_target_name or 'unknown'}'")
                expanded = game_handler.expand_rule(analysis_result)
                logger.debug(f"Successfully expanded rule for {target_type} '{rule_target_name or 'unknown'}'")
                return expanded
            else:
                logger.debug(f"safe_expand_rule: Runtime analysis failed or returned error for '{rule_target_name or 'unknown'}'")
                logger.warning(f"Failed to analyze or expand rule for {target_type} '{rule_target_name or 'unknown'}' using runtime analysis.")
                return None # Return None on failure
                
        except Exception as e:
            logger.error(f"Error analyzing/expanding rule for {target_type} '{rule_target_name or 'unknown'}': {e}")
            logger.exception("Traceback:")
        return None

    def extract_type_value(type_obj):
        """Extract clean type value from region type objects."""
        # If it's already an integer or string, return it directly
        if isinstance(type_obj, int) or isinstance(type_obj, str) and type_obj.isdigit():
            return int(type_obj)
        
        # If it has a value attribute (like an enum might), use that
        if hasattr(type_obj, 'value'):
            return type_obj.value
        
        # Try to extract value if it's in the format "Type(1)"
        str_rep = str(type_obj)
        if '(' in str_rep and ')' in str_rep:
            type_value = str_rep.split('(', 1)[1].split(')', 1)[0]
            if type_value.isdigit():
                return int(type_value)
        
        # Default: convert to string
        return str(type_obj)

    try:
        regions_data = {}
        logger.debug(f"Getting game helpers for {multiworld.game[player]}")
        # Different games have different levels of rule analysis support
        # ALTTP has detailed helper expansion, while other games may preserve more helper nodes
        game_handler = get_game_export_handler(multiworld.game[player])
        logger.debug("Successfully got game helpers")
        
        logger.debug("Getting player regions")
        player_regions = [
            region for region in multiworld.get_regions() 
            if region.player == player
        ]
        logger.debug(f"Successfully found {len(player_regions)} regions")

        # Create a location name to ID mapping for this player
        location_name_to_id = {}
        if player in multiworld.worlds:
            world = multiworld.worlds[player]
            if hasattr(world, 'location_id_to_name'):
                # Create a reverse mapping from name to ID
                location_name_to_id = {name: id for id, name in world.location_id_to_name.items()}
                logger.debug(f"Created location_name_to_id mapping with {len(location_name_to_id)} entries")

        # Process each region
        for region in player_regions:
            try:
                logger.debug(f"Processing region: {region.name}")
                region_data = {
                    'name': getattr(region, 'name', 'Unknown'),
                    'type': extract_type_value(getattr(region, 'type', 'Region')),
                    'player': getattr(region, 'player', player),
                    'is_light_world': getattr(region, 'is_light_world', False),
                    'is_dark_world': getattr(region, 'is_dark_world', False),
                    'entrances': [],
                    'exits': [],
                    'locations': [],
                    'dungeon': None,
                    'shop': None,
                    'time_passes': getattr(region, 'time_passes', True),
                    'provides_chest_count': getattr(region, 'provides_chest_count', True),
                    'region_rules': []
                }
                logger.debug("Successfully initialized region data")

                # Process dungeon data
                logger.debug("Processing dungeon data")
                if hasattr(region, 'dungeon') and region.dungeon:
                    dungeon_data = {
                        'name': getattr(region.dungeon, 'name', None),
                        'regions': [],
                        'boss': None,
                        'medallion_check': None
                    }
                    
                    if hasattr(region.dungeon, 'regions'):
                        dungeon_data['regions'] = [r.name for r in region.dungeon.regions]
                    
                    if hasattr(region.dungeon, 'boss') and region.dungeon.boss:
                        dungeon_data['boss'] = {
                            'name': getattr(region.dungeon.boss, 'name', None),
                            'defeat_rule': safe_expand_rule(
                                game_handler,
                                getattr(region.dungeon.boss, 'can_defeat', None),
                                getattr(region.dungeon.boss, 'name', None),
                                target_type='Boss'
                            )
                        }
                    
                    if hasattr(region.dungeon, 'medallion_check'):
                        dungeon_name = getattr(region.dungeon, 'name', 'UnknownDungeon')
                        dungeon_data['medallion_check'] = safe_expand_rule(
                            game_handler,
                            region.dungeon.medallion_check,
                            f"{dungeon_name} Medallion Check",
                            target_type='DungeonMedallion'
                        )
                    
                    region_data['dungeon'] = dungeon_data
                    logger.debug("Successfully processed dungeon data")

                # Process shop data
                logger.debug("Processing shop data")
                if hasattr(region, 'shop') and region.shop:
                    shop_inventory = []
                    if hasattr(region.shop, 'inventory'):
                        for item in region.shop.inventory:
                            try:
                                inventory_item = {
                                    'item': getattr(item, 'name', None),
                                    'price': getattr(item, 'price', 0),
                                    'max': getattr(item, 'max', 0),
                                    'replacement': None,
                                    'replacement_price': None
                                }
                                if hasattr(item, 'replacement') and item.replacement:
                                    inventory_item['replacement'] = item.replacement.name
                                    inventory_item['replacement_price'] = getattr(item, 'replacement_price', 0)
                                shop_inventory.append(inventory_item)
                                logger.debug(f"Successfully processed shop item: {inventory_item['item']}")
                            except Exception as e:
                                logger.error(f"Error processing shop inventory item: {str(e)}")

                    region_data['shop'] = {
                        'type': getattr(region.shop, 'type', None),
                        'inventory': shop_inventory,
                        'locked': getattr(region.shop, 'locked', False),
                        'region_name': getattr(region.shop, 'region_name', None),
                        'location_name': getattr(region.shop, 'location_name', None)
                    }
                    logger.debug("Successfully processed shop data")

                # Process entrances
                logger.debug("Processing entrances")
                if hasattr(region, 'entrances'):
                    for entrance in region.entrances:
                        try:
                            expanded_rule = None
                            entrance_name = getattr(entrance, 'name', None)
                            if hasattr(entrance, 'access_rule') and entrance.access_rule:
                                expanded_rule = safe_expand_rule(
                                    game_handler,
                                    entrance.access_rule,
                                    entrance_name,
                                    target_type='Entrance'
                                )
                            
                            entrance_data = {
                                'name': entrance_name,
                                'parent_region': getattr(entrance.parent_region, 'name', None) if hasattr(entrance, 'parent_region') else None,
                                'access_rule': expanded_rule,
                                'connected_region': getattr(entrance.connected_region, 'name', None) if hasattr(entrance, 'connected_region') else None,
                                'reverse': getattr(entrance, 'reverse', 'name', None) if hasattr(entrance, 'reverse') else None,
                                'assumed': getattr(entrance, 'assumed', False),
                                'type': getattr(entrance, 'type', 'Entrance'),
                            }
                            region_data['entrances'].append(entrance_data)
                            logger.debug(f"Successfully processed entrance: {entrance_data['name']}")
                        except Exception as e:
                            logger.error(f"Error processing entrance {getattr(entrance, 'name', 'Unknown')}: {str(e)}")

                # Process exits
                logger.debug("Processing exits")
                if hasattr(region, 'exits'):
                    for exit in region.exits:
                        try:
                            expanded_rule = None
                            exit_name = getattr(exit, 'name', None)
                            if hasattr(exit, 'access_rule') and exit.access_rule:
                                expanded_rule = safe_expand_rule(
                                    game_handler,
                                    exit.access_rule,
                                    exit_name,
                                    target_type='Exit'
                                )
                            
                            exit_data = {
                                'name': exit_name,
                                'connected_region': getattr(exit.connected_region, 'name', None) if hasattr(exit, 'connected_region') else None,
                                'access_rule': expanded_rule,
                                'type': getattr(exit, 'type', 'Exit')
                            }
                            region_data['exits'].append(exit_data)
                            logger.debug(f"Successfully processed exit: {exit_data['name']}")
                        except Exception as e:
                            logger.error(f"Error processing exit {getattr(exit, 'name', 'Unknown')}: {str(e)}")

                # Process locations
                logger.debug("Processing locations")
                if hasattr(region, 'locations'):
                    for location in region.locations:
                        try:
                            location_name = getattr(location, 'name', None)
                            
                            # Process access and item rules
                            access_rule_result = None
                            item_rule_result = None
                            
                            if hasattr(location, 'access_rule') and location.access_rule:
                                access_rule_result = safe_expand_rule(
                                    game_handler,
                                    location.access_rule,
                                    location_name,
                                    target_type='Location'
                                )
                                
                            if hasattr(location, 'item_rule') and location.item_rule:
                                item_rule_result = safe_expand_rule(
                                    game_handler,
                                    location.item_rule,
                                    f"{location_name} Item Rule",
                                    target_type='LocationItemRule'
                                )
                            
                            location_data = {
                                'name': location_name,
                                'id': location_name_to_id.get(location_name, None),  # Add location ID from mapping
                                'crystal': getattr(location, 'crystal', None),
                                'access_rule': access_rule_result,
                                'item_rule': item_rule_result,
                                'progress_type': extract_type_value(getattr(location, 'progress_type', None)),
                                'locked': getattr(location, 'locked', False),
                                'item': None
                            }
                            
                            if hasattr(location, 'item') and location.item:
                                location_data['item'] = {
                                    'name': getattr(location.item, 'name', None),
                                    'player': getattr(location.item, 'player', None),
                                    'advancement': getattr(location.item, 'advancement', False),
                                    'priority': getattr(location.item, 'priority', None),
                                    'type': extract_type_value(getattr(location.item, 'type', None))
                                }
                            
                            region_data['locations'].append(location_data)
                            logger.debug(f"Successfully processed location: {location_data['name']}")
                        except Exception as e:
                            logger.error(f"Error processing location {getattr(location, 'name', 'Unknown')}: {str(e)}")

                # Process region rules
                logger.debug("Processing region rules")
                if hasattr(region, 'region_rules'):
                    for i, rule in enumerate(region.region_rules):
                        try:
                            # Construct a target name for region rules
                            rule_target_name = f"{region.name} Rule {i+1}"
                            expanded_rule = safe_expand_rule(
                                game_handler,
                                rule,
                                rule_target_name,
                                target_type='RegionRule'
                            )
                            if expanded_rule:
                                region_data['region_rules'].append(expanded_rule)
                                logger.debug(f"Successfully processed region rule {i+1}")
                        except Exception as e:
                            logger.error(f"Error processing region rule: {str(e)}")

                regions_data[region.name] = region_data
                logger.debug(f"Successfully completed processing region: {region.name}")

            except Exception as e:
                logger.error(f"Error processing region {getattr(region, 'name', 'Unknown')}: {str(e)}")
                logger.exception("Full traceback:")
                continue

        logger.debug(f"Successfully finished process_regions for player {player}")
        return regions_data

    except Exception as e:
        logger.error(f"Error in process_regions: {str(e)}")
        logger.exception("Full traceback:")
        raise

def process_items(multiworld, player: int) -> Dict[str, Any]:
    """Process item data including progression flags and capacity information."""
    items_data = {}
    world = multiworld.worlds[player]
    game_name = multiworld.game[player]
    game_handler = get_game_export_handler(game_name) # Get game handler
    
    # 1. Start with game-specific item data from the handler
    try:
        items_data = game_handler.get_item_data(world)
        if not items_data:
             logger.warning(f"Handler for {game_name} returned no item data. Item export might be incomplete.")
    except Exception as e:
        logger.error(f"Error getting game-specific item data for {game_name}: {e}")
        items_data = {} # Start empty if handler fails

    # 2. Layer in base item IDs and groups from world.item_id_to_name
    for item_id, item_name in getattr(world, 'item_id_to_name', {}).items():
        if item_name not in items_data:
            # If item is in ID map but not handler data, create a basic entry
            logger.warning(f"Item '{item_name}' found in item_id_to_name but not in handler data for {game_name}. Creating basic entry.")
            items_data[item_name] = {
                'name': item_name,
                'id': item_id,
                'groups': [],
                'advancement': False, 'priority': False, 'useful': False, 'trap': False, 'event': False,
                'type': None, 'max_count': 1
            }
        else:
            # Ensure the ID from the world map is added if missing
            if items_data[item_name].get('id') is None:
                items_data[item_name]['id'] = item_id
        
        # Add groups from world.item_name_groups if they aren't already present
        base_groups = [
            group_name for group_name, items in getattr(world, 'item_name_groups', {}).items() 
            if item_name in items
        ]
        if item_name in items_data:
            # Ensure groups key exists
            if 'groups' not in items_data[item_name] or not isinstance(items_data[item_name]['groups'], list):
                 items_data[item_name]['groups'] = []
                 
            existing_groups = set(items_data[item_name]['groups'])
            new_groups_added = False
            for group in base_groups:
                if group not in existing_groups:
                    items_data[item_name]['groups'].append(group)
                    existing_groups.add(group)
                    new_groups_added = True
            if new_groups_added:
                 items_data[item_name]['groups'].sort()

    # 3. Update classification flags from placed items (use values from placed items if not set by handler)
    for location in multiworld.get_locations(player):
        if location.item and location.item.name in items_data:
            item_data = items_data[location.item.name]
            # Only update flags if they are still default (False)
            if not item_data.get('advancement'):
                item_data['advancement'] = getattr(location.item, 'advancement', False)
            if not item_data.get('priority'):
                item_data['priority'] = getattr(location.item, 'priority', False)
            if not item_data.get('useful'):
                 item_data['useful'] = getattr(location.item, 'useful', False)
            if not item_data.get('trap'):
                 item_data['trap'] = getattr(location.item, 'trap', False)
            # Event flag likely comes from type, less critical to update here unless specific logic requires it

    # 4. Get and apply game-specific max counts
    try:
        game_max_counts = game_handler.get_item_max_counts(world)
        for item_name, max_count in game_max_counts.items():
            if item_name in items_data:
                items_data[item_name]['max_count'] = max_count
            else:
                 logger.warning(f"Item '{item_name}' found in max counts for {game_name} but not in items_data.")
    except Exception as e:
        logger.error(f"Error getting game-specific max counts for {game_name}: {e}")

    return items_data

def process_item_groups(multiworld, player: int) -> List[str]:
    """Get item groups for this player."""
    world = multiworld.worlds[player]
    if hasattr(world, 'item_name_groups'):
        return sorted(world.item_name_groups.keys())
    return []

def process_progression_mapping(multiworld, player: int) -> Dict[str, Any]:
    """Extract progression item mapping data using the game handler."""
    try:
        world = multiworld.worlds[player]
        game_name = multiworld.game[player]
        game_handler = get_game_export_handler(game_name)
        return game_handler.get_progression_mapping(world)
    except Exception as e:
        game_name = multiworld.game.get(player, "Unknown")
        logger.error(f"Error getting progression mapping for game '{game_name}': {e}")
        logger.exception("Traceback:")
        return {} # Return empty on error

def cleanup_export_data(data):
    """
    Clean up specific fields in the export data that need special handling.
    This is applied after the initial serialization.
    """
    # Track the game type for each player to apply appropriate handler
    player_games = {}
    
    # Get player game mapping (needed for handler selection)
    # We need this info before cleaning settings, so iterate over settings first
    # even if settings themselves aren't cleaned until later
    if 'settings' in data and isinstance(data['settings'], dict):
        for player_id, settings_data in data['settings'].items():
            if isinstance(settings_data, dict) and 'game' in settings_data:
                player_games[player_id] = settings_data['game']
            else:
                # Attempt to get game name from game_info as a fallback
                if 'game_info' in data and player_id in data['game_info'] and 'name' in data['game_info'][player_id]:
                    player_games[player_id] = data['game_info'][player_id]['name']
                else:
                    logger.warning(f"Could not determine game for player {player_id} in cleanup")
                    player_games[player_id] = "unknown" # Default if not found
                    
    # Ensure game_info is properly structured (this might still be useful)
    if 'game_info' in data:
        for player_id, game_info in data['game_info'].items():
            # Make sure game_info has the game name
            if 'name' not in game_info and player_id in player_games:
                game_info['name'] = player_games[player_id]
            
            # Ensure rule_format exists
            if 'rule_format' not in game_info:
                game_info['rule_format'] = {"version": "1.0"}

    # Clean up settings fields
    if 'settings' in data:
        for player, settings in data['settings'].items():
            if not isinstance(settings, dict) or 'error' in settings: # Skip if not dict or already an error
                continue
            game = player_games.get(player, "unknown") # Get game name retrieved earlier
            game_handler = get_game_export_handler(game)
            try:
                # Delegate cleanup to the specific handler
                # Pass a copy to avoid modifying the original dict used elsewhere if cleanup fails partially
                cleaned_settings = game_handler.cleanup_settings(settings.copy())
                data['settings'][player] = cleaned_settings # Update with cleaned settings
            except Exception as e:
                logger.error(f"Error cleaning settings via handler for player {player} ({game}): {e}")
                # Keep original settings in case of error during cleanup

    # Clean up region types
    if 'regions' in data:
        for player, regions in data['regions'].items():
            for region_name, region in regions.items():
                # Convert region type to int if possible
                if 'type' in region and isinstance(region['type'], str):
                    if region['type'].isdigit():
                        region['type'] = int(region['type'])
                
                # Clean up location progress_type
                if 'locations' in region:
                    for location in region['locations']:
                        if 'progress_type' in location and isinstance(location['progress_type'], str):
                            if location['progress_type'].isdigit():
                                location['progress_type'] = int(location['progress_type'])
    
    return data

def export_test_data(multiworld, access_pool, output_dir, filename_base="test_output"):
    """
    Exports rules and test data files for frontend testing.

    Args:
        multiworld: MultiWorld instance containing game rules
        access_pool: List of access test cases
        output_dir: Directory to write output files
        filename_base: Base name for output files

    Returns:
        bool: True if export successful
    """

    import os
    os.makedirs(output_dir, exist_ok=True)

    # Set the filename base to the caller function name two levels up
    # This is the name of the test that we're currently running
    try:
        # Get the caller two levels up in the stack (the test function calling this export function)
        caller = inspect.stack()[2]
        if caller and hasattr(caller, 'function') and caller.function:
            # Use the test function name as the filename base
            filename_base = caller.function
            caller_filename = caller.filename
            
            # Extract the directory name containing the test file
            # Example path: [..., 'alttp', 'test', 'vanilla', 'TestLightWorld.py']
            # We want 'vanilla', which is the directory containing the test file
            parent_directory = os.path.basename(os.path.dirname(os.path.abspath(caller_filename)))
            logger.debug(f"Caller function: {caller.function}, filename: {caller_filename}, parent_directory: {parent_directory}") # Use standard logger
        else:
            logger.warning("Could not determine caller function name, using default filename base")
            filename_base = filename_base or "test_output"
    except IndexError as e:
        # Specific handling for stack inspection errors
        logger.error(f"Stack inspection error (not enough frames): {e}")
        filename_base = filename_base or "test_output"
    except Exception as e:
        # General error handling
        logger.error(f"Error getting caller information: {e}")
        filename_base = filename_base or "test_output"

    # Create or update test_files.json to track test files
    test_files_path = os.path.join(output_dir, "test_files.json")
    test_files_data = {}
    
    # Load existing test files data if it exists
    if os.path.exists(test_files_path):
        try:
            with open(test_files_path, 'r', encoding='utf-8') as f:
                test_files_data = json.load(f)
        except json.JSONDecodeError:
            logger.warning(f"Could not parse existing test_files.json, creating new file")
        except Exception as e:
            logger.error(f"Error reading test_files.json: {e}")
    
    # Add current filename_base as a subentry of parent_directory
    if parent_directory not in test_files_data:
        test_files_data[parent_directory] = {}
    
    if filename_base not in test_files_data[parent_directory]:
        test_files_data[parent_directory][filename_base] = True
        
        # Write updated test files data
        try:
            with open(test_files_path, 'w', encoding='utf-8') as f:
                json.dump(test_files_data, f, indent=2)
            logger.debug(f"Updated test_files.json with {parent_directory}/{filename_base}")
        except Exception as e:
            logger.error(f"Error writing to test_files.json: {e}")
    
    # Export rules data with explicit region connections
    export_data = prepare_export_data(multiworld)

    # Create a subdirectory for the parent_directory if it doesn't exist
    parent_dir_path = os.path.join(output_dir, parent_directory)
    os.makedirs(parent_dir_path, exist_ok=True)
    
    # CHANGED: Use parent_directory for rules file name instead of filename_base
    rules_path = os.path.join(parent_dir_path, f"{parent_directory}_rules.json")
    
    # Check for non-serializable parts before writing
    debug_export_data(export_data)
    
    # Make the export data serializable and clean
    success = False
    try:
        logger.info("Starting serialization of export data")
        
        # First pass: general serialization
        serializable_data = make_serializable(export_data)
        
        # Apply cleanup
        serializable_data = cleanup_export_data(serializable_data)
        
        # Write the data to file
        logger.info("Writing serialized data to file")
        with open(rules_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_data, f, indent=2)
        logger.info(f"Successfully wrote rules to {rules_path}")
        success = True
        
    except Exception as e:
        error_msg = f"Error serializing or writing JSON: {str(e)}"
        logger.error(error_msg)
        
        # Log the exception details
        import traceback
        logger.error(f"Exception traceback:\n{traceback.format_exc()}")
        
        # Try to save using field-by-field method as a fallback
        logger.error("Attempting to save field-by-field...")
        success = write_field_by_field(export_data, rules_path)
    
    # If we couldn't write the data, return failure
    if not success:
        logger.error("Failed to export rule data. See debug logs for details.")
        return False
    
    # Convert test cases to the format needed for JSON
    test_cases = []
    for location, access, *item_pool in access_pool:
        items = item_pool[0]
        all_except = item_pool[1] if len(item_pool) > 1 else None
        if all_except is not None:
            test_cases.append([location, access, items, all_except])
        else:
            test_cases.append([location, access, items])
    
    # Write to test cases JSON file
    test_cases_data = {"location_tests": test_cases}

    # Create a subdirectory for the parent_directory if it doesn't exist
    parent_dir_path = os.path.join(output_dir, parent_directory)
    os.makedirs(parent_dir_path, exist_ok=True)
    
    # Keep test cases files named by individual test function
    test_cases_path = os.path.join(parent_dir_path, f"{filename_base}_tests.json")

    
    try:
        with open(test_cases_path, 'w') as f:
            json.dump(test_cases_data, f, indent=2)
        logger.info(f"Successfully wrote test cases to {test_cases_path}")
    except Exception as e:
        error_msg = f"Error writing test cases: {str(e)}"
        logger.error(error_msg)
        return False

    print("Export process completed.")
    return True

def export_game_rules(multiworld, output_dir: str, filename_base: str, save_presets: bool = False) -> Dict[str, str]:
    """
    Exports game rules and test data to JSON files for frontend consumption.
    Also saves a copy of rules to frontend/presets with game name as prefix if save_presets is True.
    
    Args:
        multiworld: MultiWorld instance containing game rules
        output_dir: Directory to write output files
        filename_base: Base name for output files
        save_presets: Whether to save copies of files to the presets directory
        
    Returns:
        Dict containing paths to generated files
    """
    
    os.makedirs(output_dir, exist_ok=True)

    # --- Configuration for Excluded Fields --- 
    # Add keys here to exclude them from the final JSON output (e.g., to reduce size)
    # This applies recursively to nested structures.
    EXCLUDED_FIELDS = {
        'item_rule', # Example: Exclude item rules from locations
        # 'access_rule', # Example: Uncomment to exclude access rules
    }

    def remove_excluded_fields(data, excluded_keys):
        """ Recursively remove specified keys from nested dictionaries and lists. """
        if isinstance(data, dict):
            new_dict = {}
            for key, value in data.items():
                if key not in excluded_keys:
                    new_dict[key] = remove_excluded_fields(value, excluded_keys)
            return new_dict
        elif isinstance(data, list):
            return [remove_excluded_fields(item, excluded_keys) for item in data]
        else:
            return data

    # --- Define key categories (used for both combined and individual files) ---
    # Global keys apply to the entire multiworld
    global_keys = [
        "schema_version", 
        "archipelago_version", 
        "generation_seed", "player_names", "plando_options", "world_classes"
    ] # Note: game_name is handled specially
    
    # Player-specific keys contain data nested under player IDs
    player_specific_keys = [
        'regions', 'items', 'item_groups', 'progression_mapping',
        'settings', 'start_regions', 'itempool_counts', 'game_info'
    ]

    # Prepare the combined export data for all players
    try:
        export_data = prepare_export_data(multiworld)
        # Apply serialization and cleanup - important for consistent output
        serializable_data = make_serializable(export_data)
        cleaned_data = cleanup_export_data(serializable_data)
        
    except Exception as e:
        logger.error(f"Error preparing export data: {e}")
        logger.exception("Full traceback for data preparation error:")
        # Optionally return or raise here if preparation fails
        return {} # Return empty dict if preparation fails

    # --- Determine Game Name for Combined File ---
    combined_game_name = "Unknown"
    if multiworld.game:
        unique_games = set(g for g in multiworld.game.values() if g)
        if len(unique_games) > 1:
            combined_game_name = "Multiworld"
        elif len(unique_games) == 1:
            combined_game_name = list(unique_games)[0] # Get the single game name
        else: # No valid game names found
            combined_game_name = "Unknown"
    
    # --- Build Ordered Dictionary for Combined File ---
    # Define the desired order of keys
    desired_key_order = [
        'schema_version',
        'game_name',
        'archipelago_version',
        'generation_seed',
        'player_names',
        'world_classes',
        'plando_options',
        'regions',
        'start_regions',
        'items',
        'item_groups',
        'itempool_counts',
        'progression_mapping',
        'settings',
        'game_info'
    ]

    ordered_cleaned_data = collections.OrderedDict()

    # Add keys in the desired order
    for key in desired_key_order:
        if key == 'game_name':
            # Handle game_name explicitly using the determined combined_game_name
            ordered_cleaned_data[key] = combined_game_name
        elif key in cleaned_data:
            # Ensure player-specific keys retain their player-keyed structure
            if key in player_specific_keys: 
                ordered_cleaned_data[key] = cleaned_data[key]
            else:
                # Assign other global keys directly
                ordered_cleaned_data[key] = cleaned_data[key]
        else:
            # Handle potentially missing keys (other than game_name)
            logger.warning(f"Key '{key}' not found in cleaned_data for combined export")
            pass

    # Add any keys present in cleaned_data but not in desired_key_order to the end
    # This ensures no data is lost if new top-level keys are added
    for key, value in cleaned_data.items():
        if key not in ordered_cleaned_data:
            ordered_cleaned_data[key] = value
            logger.warning(f"Key '{key}' was not in desired_key_order, added to end of combined export")

    # --- Remove Excluded Fields from Combined Data ---
    final_ordered_data = remove_excluded_fields(ordered_cleaned_data, EXCLUDED_FIELDS)

    # --- Write the combined rules file using the ordered data ---
    combined_rules_path = os.path.join(output_dir, f"{filename_base}_rules.json")
    try:
        with open(combined_rules_path, 'w', encoding='utf-8') as f:
            json.dump(final_ordered_data, f, indent=2) # Use filtered data
        logger.info(f"Successfully wrote combined rules to {combined_rules_path}")
    except Exception as e:
        logger.error(f"Error writing combined rules export file: {e}")
        raise # Re-raise the exception if combined file fails

    results = {'rules_combined': combined_rules_path} # Renamed key for clarity

    # --- Write individual player rule files ONLY if more than one player ---
    if len(multiworld.player_ids) > 1:
        # Key definitions moved to top of function
        # global_keys = [...]
        # player_specific_keys = [...]

        for player in multiworld.player_ids:
            player_str = str(player)
            
            # ** Build Ordered Dictionary for Player File **
            player_export_data = collections.OrderedDict()
            
            # Define the full desired order of keys 
            # (schema_version and game_name are handled first)
            global_keys_in_order = [
                'archipelago_version',
                'generation_seed',
                'player_names',
                'world_classes',
                'plando_options'
            ]
            player_keys_in_order = [
                'regions',
                'start_regions', 
                'items',
                'item_groups',
                'itempool_counts',
                'progression_mapping', 
                'settings',
                'game_info' 
            ]
            
            # 1. Start with schema_version
            if 'schema_version' in cleaned_data:
                 player_export_data['schema_version'] = cleaned_data['schema_version']

            # 2. Add the player's specific game_name next
            player_settings = cleaned_data.get('settings', {}).get(player_str, {})
            player_game_name = player_settings.get('game', 'Unknown')
            player_export_data['game_name'] = player_game_name
            
            # 3. Add global keys in specified order
            for key in global_keys_in_order:
                if key in cleaned_data:
                    player_export_data[key] = cleaned_data[key]
                else:
                     # Handle missing global key if necessary (e.g., log warning)
                     # logger.warning(f"Global key '{key}' missing from cleaned_data")
                     pass # Or assign a default like None or {}
            
            # 4. Add player-specific keys in specified order
            for key in player_keys_in_order:
                if key in cleaned_data and player_str in cleaned_data.get(key, {}):
                    # Assign a dictionary containing the player key and their data
                    player_export_data[key] = {player_str: cleaned_data[key][player_str]}
                else:
                    # Log if expected player-specific data is missing
                    logger.warning(f"Missing player-specific key '{key}' for player {player_str} in cleaned_data")
                    # Provide an empty dict or appropriate default based on the key type
                    player_export_data[key] = {} # Default to empty dict

            # Define player-specific filename
            player_rules_path = os.path.join(output_dir, f"{filename_base}_P{player_str}_rules.json")
            
            # --- Remove Excluded Fields from Player Data ---
            final_player_data = remove_excluded_fields(player_export_data, EXCLUDED_FIELDS)

            try:
                with open(player_rules_path, 'w', encoding='utf-8') as f:
                    json.dump(final_player_data, f, indent=2) # Use filtered data
                logger.info(f"Successfully wrote rules for player {player_str} to {player_rules_path}")
                results[f"rules_p{player_str}"] = player_rules_path # Add player file path to results
            except Exception as e:
                logger.error(f"Error writing rules export file for player {player_str}: {e}")
                # Optionally, add placeholder to results or skip adding
                results[f"rules_p{player_str}"] = f"ERROR: Failed to write file - {e}"
                # Decide if we should continue or raise if a player file fails
                # For now, we log the error and continue

    # If save_presets is False, skip the preset saving parts
    if not save_presets:
        return results

    # --- Save presets (this part remains largely the same) ---
    # It will now copy the combined file AND all individual player files
    try:
        # Determine if it's a multi-game world for naming purposes
        if not multiworld.game:
            logger.warning("No game data found in multiworld object, skipping preset save")
            return results
            
        unique_games = set(g for g in multiworld.game.values() if g) # Get unique, non-empty game names
        
        is_multi_game = len(unique_games) > 1
        
        if is_multi_game:
            game_name = "Multiworld" # Name used in descriptions
            clean_game_name = "multiworld" # Name used for the folder
            logger.info(f"Detected multi-game world ({len(unique_games)} unique games), using '{clean_game_name}' preset folder.")
        else:
            # Single game or empty game dict, use first player's game (or default)
            first_player = min(multiworld.game.keys()) if multiworld.game else 1 # Handle empty case
            game_name = multiworld.game.get(first_player, "unknown_game")
            
            if not game_name or game_name == "unknown_game":
                logger.warning(f"Could not determine valid game name for player {first_player}, skipping preset save")
                return results
            
            # Clean the single game name for use in a filename/folder
            clean_game_name = game_name.lower().replace(' ', '_').replace(':', '_')
            logger.info(f"Detected single game world ({game_name}), using '{clean_game_name}' preset folder.")

        # Determine the frontend presets directory
        presets_dir = os.path.join(os.path.dirname(__file__), '..', 'frontend', 'presets')
        os.makedirs(presets_dir, exist_ok=True)
        
        # Create game-specific directory (will be 'multiworld' or the specific game name)
        game_dir = os.path.join(presets_dir, clean_game_name)
        os.makedirs(game_dir, exist_ok=True)
        
        # Create a folder for this specific preset using filename_base
        preset_dir = os.path.join(game_dir, filename_base)

        # --- Compare existing preset files with new files --- 
        needs_update = True # Assume update is needed unless proven otherwise
        if os.path.exists(preset_dir):
            try:
                output_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.json') and os.path.isfile(os.path.join(output_dir, f))])
                preset_files_existing = sorted([f for f in os.listdir(preset_dir) if f.endswith('.json') and os.path.isfile(os.path.join(preset_dir, f))])

                if set(output_files) == set(preset_files_existing):
                    # File lists match, now compare content
                    all_match = True
                    for filename in output_files:
                        output_path = os.path.join(output_dir, filename)
                        preset_path = os.path.join(preset_dir, filename)
                        try:
                            with open(output_path, 'r', encoding='utf-8') as f_out, open(preset_path, 'r', encoding='utf-8') as f_pre:
                                output_json = json.load(f_out)
                                preset_json = json.load(f_pre)
                                if output_json != preset_json:
                                    all_match = False
                                    logger.info(f"Preset content mismatch found in {filename}. Update needed.")
                                    break # No need to check further files
                        except (FileNotFoundError, json.JSONDecodeError, Exception) as cmp_e:
                            logger.warning(f"Error comparing preset file {filename}: {cmp_e}. Assuming update needed.")
                            all_match = False # Treat comparison errors as needing update
                            break
                    
                    if all_match:
                        needs_update = False # All files exist and content matches
                        logger.info(f"Preset {preset_dir} is up-to-date. Skipping file copy.")
                else:
                    logger.info(f"Preset file list mismatch for {preset_dir}. Update needed.")

            except Exception as check_e:
                 logger.error(f"Error checking existing preset directory {preset_dir}: {check_e}. Proceeding with update.")
                 needs_update = True # Fallback to updating if check fails
        else:
             logger.info(f"Preset directory {preset_dir} does not exist. Creating and copying files.")
             needs_update = True # Directory doesn't exist, definitely need to create/copy

        files_copied = 0
        preset_files_after_copy = [] # Store the list of files *after* potential copy

        # --- Create/Update Preset Directory and Copy Files (only if needed) --- 
        if needs_update:
            logger.info(f"Updating preset directory: {preset_dir}")
            if not os.path.exists(preset_dir):
                os.makedirs(preset_dir)
                logger.info(f"Created new preset directory: {preset_dir}")
            else:
                logger.info(f"Clearing existing preset directory: {preset_dir}")
                for item in os.listdir(preset_dir):
                    item_path = os.path.join(preset_dir, item)
                    if os.path.isfile(item_path):
                        try:
                            os.remove(item_path)
                        except Exception as remove_e:
                            logger.error(f"Error removing file {item_path}: {remove_e}")
                    elif os.path.isdir(item_path):
                        try:
                            shutil.rmtree(item_path)
                        except Exception as rmtree_e:
                            logger.error(f"Error removing directory {item_path}: {rmtree_e}")

            # Copy all files from output_dir to preset_dir
            # This will now include the combined file and all player-specific files
            for file_name in os.listdir(output_dir):
                src_file = os.path.join(output_dir, file_name)
                if os.path.isfile(src_file):
                    try:
                        dst_file = os.path.join(preset_dir, file_name)
                        shutil.copy2(src_file, dst_file)
                        files_copied += 1
                    except Exception as copy_e:
                         logger.error(f"Error copying file {src_file} to {preset_dir}: {copy_e}")

            logger.info(f"Copied {files_copied} files to preset directory {preset_dir}")

            # Get list of files in the preset directory *after* copying
            try:
                preset_files_after_copy = sorted([f for f in os.listdir(preset_dir) if os.path.isfile(os.path.join(preset_dir, f))])
            except Exception as list_e:
                logger.error(f"Error listing files in updated preset directory {preset_dir}: {list_e}")
                preset_files_after_copy = [] # Fallback to empty list

            # --- Update preset_files.json Index (only if update occurred OR first time) ---
            # We update the index even if files weren't copied, to ensure seed/game info is current,
            # but the file list itself comes from the actual state of the directory.
            preset_index_path = os.path.join(presets_dir, 'preset_files.json')
            preset_index = {}
            
            # Load existing index if available
            if os.path.exists(preset_index_path):
                try:
                    with open(preset_index_path, 'r', encoding='utf-8') as f:
                        preset_index = json.load(f)
                except json.JSONDecodeError:
                    logger.warning("Could not parse existing preset_files.json, creating new file")
                except Exception as read_e:
                     logger.error(f"Error reading preset_files.json: {read_e}")
                     preset_index = {} # Reset index on error

            # Check if the entry needs to be added or updated
            update_index_entry = True # Default to updating the index entry
            if not needs_update and clean_game_name in preset_index and filename_base in preset_index[clean_game_name].get("folders", {}):
                 # If no file copy happened AND the entry already exists, maybe skip index update?
                 # For now, let's update anyway to refresh seed/game info even if files are identical.
                 # To *truly* skip if identical, we'd need to compare seed/game info too.
                 logger.info(f"Preset files for {clean_game_name}/{filename_base} were identical, but updating index entry with current seed/game info.")

            # Initialize game entry if it doesn't exist
            if clean_game_name not in preset_index:
                preset_index[clean_game_name] = {
                    "name": game_name, # Store the original game name
                    "folders": {}
                }
            # Make sure folders key exists and is a dictionary
            elif "folders" not in preset_index[clean_game_name] or not isinstance(preset_index[clean_game_name].get("folders"), dict):
                preset_index[clean_game_name]["folders"] = {}
            
            # --- Prepare player game data ---
            player_game_data = []
            for player_id in multiworld.player_ids:
                player_name = multiworld.player_name.get(player_id, f"Player {player_id}")
                game_for_player = multiworld.game.get(player_id, "Unknown Game")
                player_game_data.append({
                    "player": player_id,
                    "name": player_name,
                    "game": game_for_player
                })

            # Add or update the folder entry with the new structure
            # Use preset_files_after_copy which reflects the actual files in the directory
            preset_index[clean_game_name]["folders"][filename_base] = {
                "seed": multiworld.seed, # ADDED
                "games": player_game_data, # ADDED
                "files": preset_files_after_copy # Use the actual list from the directory
            }
            
            logger.info(f"Updated preset_files.json index for {clean_game_name}/{filename_base} with {len(preset_files_after_copy)} files, seed, and game info")
            
            # Write updated index
            try:
                with open(preset_index_path, 'w', encoding='utf-8') as f:
                    # Keep the original insertion order (requires Python 3.7+ for dict order)
                    # For broader compatibility, could sort outer keys if needed:
                    # sorted_index = dict(sorted(preset_index.items()))
                    json.dump(preset_index, f, indent=2)
            except Exception as write_e:
                logger.error(f"Error writing updated preset_files.json: {write_e}")

    except Exception as e:
        # Log but don't fail the entire export if preset saving fails
        logger.error(f"Error during preset saving logic: {e}")
        logger.exception("Exception details during preset saving:")

    return results # Return results regardless of preset save success/failure