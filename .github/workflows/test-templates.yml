name: Test Templates

on:
  # Run on push to main branch
  push:
    branches:
      - main
    paths:
      - 'Players/Templates/*.yaml'
      - 'scripts/test/**'
      - '.github/workflows/test-templates.yml'

  # Run on pull requests
  pull_request:
    paths:
      - 'Players/Templates/*.yaml'
      - 'scripts/test/**'
      - '.github/workflows/test-templates.yml'

  # Allow manual triggering
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Which test suite to run'
        required: true
        default: 'quick'
        type: choice
        options:
          - quick
          - full

permissions:
  contents: write  # Allow pushing commits and creating branches

jobs:
  # Quick test - runs on every PR/push and manual trigger with 'quick'
  quick-test:
    name: Quick Test (ALTTP)
    runs-on: ubuntu-latest
    timeout-minutes: 30

    # Run on PR/push, or on manual dispatch with 'quick' selected
    if: github.event_name != 'workflow_dispatch' || github.event.inputs.test_suite == 'quick'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '~3.12.7'
          check-latest: true

      - name: Install system dependencies
        run: |
          sudo apt update
          sudo apt -y install python3-gi libgirepository1.0-dev

      - name: Create virtual environment and install dependencies
        run: |
          python -m venv .venv
          source .venv/bin/activate
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python ModuleUpdate.py --yes

      - name: Generate template files
        run: |
          source .venv/bin/activate
          python -c "from Options import generate_yaml_templates; generate_yaml_templates('Players/Templates')"

      - name: Configure host settings
        run: |
          source .venv/bin/activate
          python Launcher.py --update_settings
          python scripts/setup/update_host_settings.py minimal-spoilers

      - name: Install Node.js dependencies
        run: npm install

      - name: Install Playwright browsers
        run: |
          source .venv/bin/activate
          npx playwright install chromium

      - name: Run quick test (with HTTP server)
        run: |
          # Start HTTP server in background
          python -m http.server 8000 &
          SERVER_PID=$!
          sleep 2

          # Run template test
          source .venv/bin/activate
          python scripts/test/test-all-templates.py --include-list "A Link to the Past.yaml" -p

          # Stop HTTP server
          kill $SERVER_PID || true
          sleep 1

      - name: Run regression tests
        run: |
          npm test -- --mode=test-regression

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: quick-test-results
          path: |
            scripts/output/
            docs/json/developer/test-results/
          retention-days: 7

  # Full test suite - only runs on manual workflow_dispatch with 'full'
  full-test:
    name: Full Test - ${{ matrix.test_name }}
    runs-on: ubuntu-latest
    timeout-minutes: 180

    # Only run on manual workflow dispatch with 'full' selected
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.test_suite == 'full'

    strategy:
      fail-fast: false
      matrix:
        include:
          # Minimal spoiler test for all templates, seed 1 only
          - test_name: "Minimal Spoiler (Seed 1)"
            host_config: "minimal-spoilers"
            test_command: "python scripts/test/test-all-templates.py -p"
            output_dir: "spoiler-minimal"
            output_md_file: "test-results-spoilers-minimal.md"

          # Full spoiler test for all templates, seed 1 only
          - test_name: "Full Spoiler (Seed 1)"
            host_config: "full-spoilers"
            test_command: "python scripts/test/test-all-templates.py -p"
            output_dir: "spoiler-full"
            output_md_file: "test-results-spoilers-full.md"

          # Multiplayer test for all templates
          - test_name: "Multiplayer"
            host_config: "minimal-spoilers"
            test_command: "python scripts/test/test-all-templates.py --multiplayer -p"
            output_dir: "multiplayer"
            output_md_file: "test-results-multiplayer.md"

          # Multiworld test for all templates
          - test_name: "Multiworld"
            host_config: "minimal-spoilers"
            test_command: "python scripts/test/test-all-templates.py --multiworld -p"
            output_dir: "multiworld"
            output_md_file: "test-results-multiworld.md"

          # Multitemplate test for ALTTP, minimal
          - test_name: "Multitemplate ALTTP Minimal"
            host_config: "minimal-spoilers"
            setup_multitemplate: true
            test_command: "python scripts/test/test-all-templates.py --templates-dir Players/presets/multitemplate/alttp --multitemplate -p"
            output_dir: "multitemplate-minimal"
            output_md_file: "test-results-multitemplate-minimal.md"

          # Multitemplate test for ALTTP, full
          - test_name: "Multitemplate ALTTP Full"
            host_config: "full-spoilers"
            setup_multitemplate: true
            test_command: "python scripts/test/test-all-templates.py --templates-dir Players/presets/multitemplate/alttp --multitemplate -p"
            output_dir: "multitemplate-full"
            output_md_file: "test-results-multitemplate-full.md"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '~3.12.7'
          check-latest: true

      - name: Install system dependencies
        run: |
          sudo apt update
          sudo apt -y install python3-gi libgirepository1.0-dev

      - name: Create virtual environment and install dependencies
        run: |
          python -m venv .venv
          source .venv/bin/activate
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python ModuleUpdate.py --yes

      - name: Generate template files
        run: |
          source .venv/bin/activate
          python -c "from Options import generate_yaml_templates; generate_yaml_templates('Players/Templates')"

      - name: Generate multitemplate configurations
        if: matrix.setup_multitemplate
        run: |
          source .venv/bin/activate
          python scripts/build/generate-multitemplate-configs.py

      - name: Configure host settings
        run: |
          source .venv/bin/activate
          python Launcher.py --update_settings
          python scripts/setup/update_host_settings.py ${{ matrix.host_config }}

      - name: Install Node.js dependencies
        run: npm install

      - name: Install Playwright browsers
        run: |
          source .venv/bin/activate
          npx playwright install chromium

      - name: Start HTTP server
        run: |
          python -m http.server 8000 &
          sleep 2

      - name: Run test - ${{ matrix.test_name }}
        run: |
          source .venv/bin/activate
          ${{ matrix.test_command }}

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: full-test-${{ matrix.test_name }}
          path: |
            scripts/output/${{ matrix.output_dir }}/
            docs/json/developer/test-results/${{ matrix.output_md_file }}
            docs/json/developer/test-results/test-results-summary.md
          retention-days: 14

      - name: Check for test failures
        if: always()
        run: |
          source .venv/bin/activate
          # Parse results and fail if tests failed
          python -c "
          import json
          import sys
          from pathlib import Path

          failed = False

          # Find all test results files
          for results_file in Path('scripts/output').rglob('test-results.json'):
              if results_file.stat().st_size > 0:
                  with open(results_file) as f:
                      data = json.load(f)
                      results = data.get('results', {})

                      for template, result in results.items():
                          if isinstance(result, dict):
                              # Check spoiler test (only if it exists)
                              if 'spoiler_test' in result:
                                  if result['spoiler_test'].get('pass_fail') == 'failed':
                                      print(f'❌ {template} failed spoiler test')
                                      failed = True

                              # Check multiplayer test (only if it exists)
                              if 'multiplayer_test' in result:
                                  if not result['multiplayer_test'].get('success', False):
                                      print(f'❌ {template} failed multiplayer test')
                                      failed = True

                              # Check multiworld test (only if it exists)
                              if 'multiworld_test' in result:
                                  if not result['multiworld_test'].get('success', False):
                                      print(f'❌ {template} failed multiworld test')
                                      failed = True

                              # Check generation (always check this)
                              if not result.get('generation', {}).get('success', False):
                                  print(f'❌ {template} failed generation')
                                  failed = True

          if failed:
              print('\\n❌ Some tests failed')
              sys.exit(1)
          else:
              print('✅ All tests passed!')
          "

  # Commit results job - runs after all full tests complete
  commit-results:
    name: Commit Test Results
    runs-on: ubuntu-latest
    needs: full-test
    if: always() && github.event_name == 'workflow_dispatch' && github.event.inputs.test_suite == 'full'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref }}

      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Merge test results into repository
        run: |
          # Create temporary directory for merged results
          mkdir -p /tmp/merged-results/scripts/output
          mkdir -p /tmp/merged-results/docs/json/developer/test-results

          # Copy all artifact contents to temporary location
          # Each artifact contains only its specific output_dir and specific .md file
          find artifacts -type d -path "*/scripts/output/*" ! -path "*/scripts/output" -exec sh -c 'cp -r "$1" /tmp/merged-results/scripts/output/' sh {} \;
          find artifacts -type f -path "*/docs/json/developer/test-results/*.md" -exec cp {} /tmp/merged-results/docs/json/developer/test-results/ \;

          # List what was copied
          echo "=== Copied JSON Results to temp ==="
          ls -lh /tmp/merged-results/scripts/output/*/test-results.json || echo "No JSON files found"

          echo "=== Copied Markdown Files to temp ==="
          ls -lh /tmp/merged-results/docs/json/developer/test-results/*.md || echo "No markdown files found"

      - name: Create or switch to test-results-update branch
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Fetch all branches
          git fetch origin

          # Check if branch exists remotely
          if git ls-remote --exit-code --heads origin test-results-update; then
            echo "Branch test-results-update exists, checking it out"
            git checkout test-results-update
            git pull origin test-results-update

            # Merge main into test-results-update to stay synchronized
            echo "Merging main into test-results-update..."
            # Use -X theirs strategy to auto-accept main's version for conflicts
            git merge origin/main --allow-unrelated-histories --no-edit -X theirs -m "Merge main into test-results-update before updating test results"
          else
            echo "Creating new branch test-results-update from main"
            git checkout -b test-results-update origin/main
          fi

          # Now copy the merged results from temp location
          echo "Copying merged results to repository..."
          mkdir -p scripts/output
          mkdir -p docs/json/developer/test-results

          cp -r /tmp/merged-results/scripts/output/* scripts/output/ 2>/dev/null || true
          cp /tmp/merged-results/docs/json/developer/test-results/*.md docs/json/developer/test-results/ 2>/dev/null || true

          echo "=== Files in repository after copy ==="
          ls -lh scripts/output/*/test-results.json || echo "No JSON files"
          ls -lh docs/json/developer/test-results/*.md || echo "No MD files"

      - name: Commit and push results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Add the specific files we want to commit
          # Use -f because scripts/output is in .gitignore (even though specific files are excepted)
          git add -f scripts/output/spoiler-minimal/test-results.json
          git add -f scripts/output/spoiler-full/test-results.json
          git add -f scripts/output/multiplayer/test-results.json
          git add -f scripts/output/multiworld/test-results.json
          git add -f scripts/output/multitemplate-minimal/test-results.json
          git add -f scripts/output/multitemplate-full/test-results.json
          git add docs/json/developer/test-results/*.md

          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update test results from GitHub Actions

            Generated by workflow run: ${{ github.run_id }}
            Triggered by: ${{ github.actor }}

            Test results include:
            - Minimal spoiler tests (seed 1 only)
            - Full spoiler tests (seed 1 only)
            - Multiplayer tests
            - Multiworld tests
            - Multitemplate ALTTP tests (minimal and full)
            "

            git push origin test-results-update

            echo "✅ Test results committed to test-results-update branch"
            echo "View the branch at: ${{ github.server_url }}/${{ github.repository }}/tree/test-results-update"
          fi
