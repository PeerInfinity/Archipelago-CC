#!/usr/bin/env python3
"""
Automation script to test all template files by running the generation script
and spoiler tests, collecting results in a JSON file.

This script iterates through YAML files in the Templates folder (or an alternate
path specified via command line), runs Generate.py for each template, and then
runs the spoiler test. It collects error/warning counts and test results in a
comprehensive JSON output file.
"""

import argparse
import json
import os
import re
import subprocess
import sys
import time
import urllib.request
import urllib.error
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from seed_utils import get_seed_id as compute_seed_id


def run_post_processing_scripts(project_root: str, results_file: str):
    """Run post-processing scripts to update documentation and preset files."""
    print("\n=== Running Post-Processing Scripts ===")
    
    # Script 1: Generate test chart
    print("\nGenerating test results chart...")
    chart_script = os.path.join(project_root, 'scripts', 'generate-test-chart.py')
    try:
        result = subprocess.run(
            [sys.executable, chart_script, '--input-file', results_file],
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            print("âœ“ Test chart generated successfully")
            # Show where the chart was saved
            if "Chart saved to:" in result.stdout:
                for line in result.stdout.split('\n'):
                    if "Chart saved to:" in line:
                        print(f"  {line.strip()}")
        else:
            print(f"âœ— Failed to generate test chart: {result.stderr}")
    except subprocess.TimeoutExpired:
        print("âœ— Test chart generation timed out")
    except Exception as e:
        print(f"âœ— Error running generate-test-chart.py: {e}")
    
    # Script 2: Update preset files
    print("\nUpdating preset files with test data...")
    preset_script = os.path.join(project_root, 'scripts', 'update-preset-files.py')
    try:
        result = subprocess.run(
            [sys.executable, preset_script, '--test-results', results_file],
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode == 0:
            print("âœ“ Preset files updated successfully")
            # Show summary from output
            if "Summary:" in result.stdout:
                in_summary = False
                for line in result.stdout.split('\n'):
                    if "Summary:" in line:
                        in_summary = True
                    elif in_summary and line.strip().startswith('-'):
                        print(f"  {line.strip()}")
        else:
            print(f"âœ— Failed to update preset files: {result.stderr}")
    except subprocess.TimeoutExpired:
        print("âœ— Preset files update timed out")
    except Exception as e:
        print(f"âœ— Error running update-preset-files.py: {e}")
    
    print("\n=== Post-Processing Complete ===")


def build_and_load_world_mapping(project_root: str) -> Dict[str, Dict]:
    """Build world mapping and load it."""
    mapping_file = os.path.join(project_root, 'scripts', 'data', 'world-mapping.json')
    build_script = os.path.join(project_root, 'scripts', 'build-world-mapping.py')
    
    # Always build the mapping to ensure it's current
    try:
        print("Building world mapping...")
        result = subprocess.run([sys.executable, build_script], 
                              cwd=project_root, 
                              capture_output=True, 
                              text=True)
        if result.returncode != 0:
            print(f"Warning: Failed to build world mapping: {result.stderr}")
            return {}
        else:
            print("World mapping built successfully")
    except Exception as e:
        print(f"Warning: Failed to build world mapping: {e}")
        return {}
    
    # Load the mapping
    try:
        with open(mapping_file, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        print("Warning: Could not load world mapping, using empty mapping")
        return {}


def extract_game_name_from_template(template_path: str) -> Optional[str]:
    """Extract the game name from a template YAML file."""
    try:
        import yaml
        with open(template_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
            return data.get('game')
    except (ImportError, yaml.YAMLError, FileNotFoundError, UnicodeDecodeError):
        # Fall back to normalized template name if YAML parsing fails
        return None


def get_world_info(template_file: str, templates_dir: str, world_mapping: Dict[str, Dict]) -> Dict:
    """Get world information including custom exporter/gameLogic status."""
    template_path = os.path.join(templates_dir, template_file)
    
    # Try to extract game name from YAML file
    game_name = extract_game_name_from_template(template_path)
    
    if game_name and game_name in world_mapping:
        world_info = world_mapping[game_name].copy()
        world_info['game_name_from_yaml'] = game_name
        world_info['normalized_name'] = normalize_game_name(template_file)
        return world_info
    else:
        # Fallback to normalized name
        normalized_name = normalize_game_name(template_file)
        return {
            'game_name_from_yaml': game_name,
            'normalized_name': normalized_name,
            'world_directory': None,
            'has_custom_exporter': False,
            'has_custom_game_logic': False,
            'exporter_path': None,
            'game_logic_path': None
        }


def check_virtual_environment() -> bool:
    """
    Check if the virtual environment is properly activated.
    Returns True if environment is ready, False otherwise.
    """
    # Check if VIRTUAL_ENV environment variable is set (most reliable indicator)
    if 'VIRTUAL_ENV' in os.environ:
        return True
    
    # If not, check if we can import dependencies (fallback for other setups)
    try:
        # Add the project root to Python path for imports
        project_root = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))
        if project_root not in sys.path:
            sys.path.insert(0, project_root)
        
        # Try to import a dependency that should be available
        import BaseClasses
        
        # If import works but no VIRTUAL_ENV, warn but allow to continue
        return True
    except ImportError:
        return False


def check_http_server(url: str = "http://localhost:8000", timeout: int = 5) -> bool:
    """
    Check if the HTTP server is running by attempting to connect.
    Returns True if server is reachable, False otherwise.
    """
    try:
        with urllib.request.urlopen(url, timeout=timeout) as response:
            return response.getcode() == 200
    except (urllib.error.URLError, urllib.error.HTTPError, OSError):
        return False


def get_world_directory_name_from_game_name(game_name: str) -> str:
    """
    Get the world directory name for a given game name by scanning worlds directory.
    This replicates the logic from build-world-mapping.py and exporter.py.
    """
    try:
        # Get path to worlds directory relative to this file (scripts/test-all-templates.py)
        project_root = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))
        worlds_dir = os.path.join(project_root, 'worlds')
        
        if not os.path.exists(worlds_dir):
            print(f"Warning: Worlds directory not found: {worlds_dir}")
            return game_name.lower().replace(' ', '_').replace(':', '_')
        
        # Scan each world directory
        for world_dir_name in os.listdir(worlds_dir):
            world_path = os.path.join(worlds_dir, world_dir_name)
            
            # Skip non-directories and hidden/private directories
            if not os.path.isdir(world_path) or world_dir_name.startswith('.') or world_dir_name.startswith('_'):
                continue
                
            init_file = os.path.join(world_path, '__init__.py')
            if not os.path.exists(init_file):
                continue
                
            # Extract game name from __init__.py
            try:
                with open(init_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # Look for pattern: game: ClassVar[str] = "Game Name"
                pattern = r'game:\s*ClassVar\[str\]\s*=\s*"([^"]*)"'
                match = re.search(pattern, content, re.MULTILINE)
                
                if match:
                    found_game_name = match.group(1)
                    if found_game_name == game_name:
                        return world_dir_name
                
                # Fallback pattern for single quotes
                pattern = r'game:\s*ClassVar\[str\]\s*=\s*\'([^\']*)\''
                match = re.search(pattern, content, re.MULTILINE)
                
                if match:
                    found_game_name = match.group(1)
                    if found_game_name == game_name:
                        return world_dir_name
                
                # Fallback: look for simpler pattern: game = "Game Name"
                pattern = r'game\s*=\s*"([^"]*)"'
                match = re.search(pattern, content, re.MULTILINE)
                
                if match:
                    found_game_name = match.group(1)
                    if found_game_name == game_name:
                        return world_dir_name
                
                # Fallback pattern for single quotes
                pattern = r'game\s*=\s*\'([^\']*)\''
                match = re.search(pattern, content, re.MULTILINE)
                
                if match:
                    found_game_name = match.group(1)
                    if found_game_name == game_name:
                        return world_dir_name
                        
            except (IOError, UnicodeDecodeError):
                continue
        
        # If no matching world found, fall back to old logic
        return game_name.lower().replace(' ', '_').replace(':', '_')
        
    except Exception as e:
        print(f"Error finding world directory for game '{game_name}': {e}")
        return game_name.lower().replace(' ', '_').replace(':', '_')


def normalize_game_name(template_name: str) -> str:
    """Convert template filename to world directory name format."""
    # Remove .yaml extension to get the game name (handle both .yaml and .yml)
    game_name = template_name
    if game_name.endswith('.yaml'):
        game_name = game_name[:-5]
    elif game_name.endswith('.yml'):
        game_name = game_name[:-4]
    # Use the same logic as the exporter to find the world directory name
    return get_world_directory_name_from_game_name(game_name)


def count_errors_and_warnings(text: str) -> Tuple[int, int, Optional[str], Optional[str]]:
    """
    Count occurrences of 'error' and 'warning' in text (case insensitive).
    Ignores lines that start with "[SKIP]", contain "Error Logs:", or "No errors detected" to avoid false positives.
    Returns tuple of (error_count, warning_count, first_error_line, first_warning_line).
    """
    lines = text.split('\n')
    error_count = 0
    warning_count = 0
    first_error_line = None
    first_warning_line = None
    
    for line in lines:
        line_stripped = line.strip()
        line_lower = line_stripped.lower()
        
        # Skip lines that are false positives
        if (line_stripped.startswith('[SKIP]') or 
            'error logs:' in line_lower or
            'no errors detected' in line_lower):
            continue
            
        if 'error' in line_lower:
            error_count += 1
            if first_error_line is None:
                first_error_line = line_stripped
        if 'warning' in line_lower:
            warning_count += 1
            if first_warning_line is None:
                first_warning_line = line_stripped
    
    return error_count, warning_count, first_error_line, first_warning_line


def run_command(cmd: List[str], cwd: str = None, timeout: int = 300, env: Dict = None) -> Tuple[int, str, str]:
    """
    Run a command and return (return_code, stdout, stderr).
    """
    try:
        result = subprocess.run(
            cmd,
            cwd=cwd,
            env=env,
            capture_output=True,
            text=True,
            timeout=timeout
        )
        return result.returncode, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return -1, "", "Command timed out"
    except Exception as e:
        return -1, "", str(e)


def count_total_spheres(spheres_log_path: str) -> float:
    """
    Get the highest sphere_index from spheres_log.jsonl file.
    Returns the sphere_index value from the last line in the file.
    """
    try:
        if not os.path.exists(spheres_log_path):
            return 0
        
        with open(spheres_log_path, 'r') as f:
            last_sphere = 0
            for line in f:
                line = line.strip()
                if line:
                    try:
                        sphere_data = json.loads(line)
                        sphere_index = sphere_data.get('sphere_index', 0)
                        # Convert to float to handle values like "1.1", "2.3", etc.
                        if isinstance(sphere_index, str):
                            last_sphere = float(sphere_index)
                        else:
                            last_sphere = float(sphere_index)
                    except (json.JSONDecodeError, ValueError):
                        continue
            return last_sphere
    except (IOError, OSError):
        return 0


def parse_playwright_analysis(analysis_text: str) -> Dict:
    """
    Parse playwright-analysis.txt to extract test results.
    """
    result = {
        'pass_fail': 'unknown',
        'sphere_reached': 0,
        'total_spheres': 0,
        'error_count': 0,
        'warning_count': 0,
        'first_error_line': None,
        'first_warning_line': None
    }
    
    # Count errors and warnings
    error_count, warning_count, first_error, first_warning = count_errors_and_warnings(analysis_text)
    result['error_count'] = error_count
    result['warning_count'] = warning_count
    result['first_error_line'] = first_error
    result['first_warning_line'] = first_warning
    
    # Parse for sphere information and pass/fail status
    lines = analysis_text.split('\n')
    for line in lines:
        line_stripped = line.strip()
        line_upper = line_stripped.upper()
        
        # Look for pass/fail status - check for [PASS] or [FAIL] in test results
        if '[PASS]' in line_upper:
            result['pass_fail'] = 'passed'
        elif '[FAIL]' in line_upper:
            result['pass_fail'] = 'failed'
        elif 'PASSED:' in line_upper or 'FAILED:' in line_upper:
            if 'PASSED:' in line_upper:
                result['pass_fail'] = 'passed'
            else:
                result['pass_fail'] = 'failed'
        elif 'NO ERRORS DETECTED' in line_upper:
            result['pass_fail'] = 'passed'
        
        # Look for sphere information
        sphere_match = re.search(r'sphere\s+(\d+(?:\.\d+)?)', line_stripped.lower())
        if sphere_match:
            result['sphere_reached'] = float(sphere_match.group(1))
        
        total_match = re.search(r'(\d+)\s+total\s+spheres?', line_stripped.lower())
        if total_match:
            result['total_spheres'] = int(total_match.group(1))
        
        # Alternative patterns for total spheres
        if 'spheres' in line_stripped.lower() and '/' in line_stripped:
            parts = line_stripped.split('/')
            if len(parts) >= 2:
                try:
                    total = int(re.findall(r'\d+', parts[1])[0])
                    result['total_spheres'] = total
                except (IndexError, ValueError):
                    pass
    
    return result


def load_existing_results(results_file: str) -> Dict:
    """Load existing results file or create empty structure."""
    if os.path.exists(results_file):
        try:
            with open(results_file, 'r') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            pass

    return {
        'metadata': {
            'created': datetime.now().isoformat(),
            'last_updated': datetime.now().isoformat(),
            'script_version': '1.0.0'
        },
        'results': {}
    }


def merge_results(existing_results: Dict, new_results: Dict, templates_tested: List[str], update_metadata: bool = True) -> Dict:
    """
    Merge new test results into existing results.
    Only updates entries for templates/seeds that were tested in the current run.
    Preserves all other existing results.

    For seed range tests, merges at the individual seed level - if you test seeds 1-10
    and then retest just seed 1, only seed 1's results are updated.

    Args:
        existing_results: The existing results to merge into
        new_results: The new results from this run
        templates_tested: List of template names tested in this run
        update_metadata: If True, updates metadata fields like batch processing time
    """
    merged = existing_results.copy()

    # Only update metadata if requested (typically only for full runs, not --include-list runs)
    if update_metadata:
        merged['metadata']['last_updated'] = new_results['metadata']['last_updated']

        # Copy over batch processing time metadata from new results if present
        for key in ['batch_processing_time_seconds', 'batch_processing_time_minutes', 'average_time_per_template_seconds']:
            if key in new_results['metadata']:
                merged['metadata'][key] = new_results['metadata'][key]
    else:
        # Always update last_updated timestamp, but not the batch processing metrics
        merged['metadata']['last_updated'] = new_results['metadata']['last_updated']

    # Merge results: only update entries for templates tested in this run
    if 'results' not in merged:
        merged['results'] = {}

    for template_name in templates_tested:
        if template_name not in new_results['results']:
            continue

        new_result = new_results['results'][template_name]

        # Check if existing result is a seed range (has 'individual_results' key)
        existing_has_seed_range = (template_name in merged['results'] and
                                   'individual_results' in merged['results'][template_name])

        # Check if this is a seed range result (has 'individual_results' key)
        if 'individual_results' in new_result:
            # New result is a seed range result - merge at the seed level
            if existing_has_seed_range:
                # Template already exists with seed range results - merge individual seeds
                existing_template = merged['results'][template_name]

                # Merge individual seed results
                if 'individual_results' not in existing_template:
                    existing_template['individual_results'] = {}

                for seed, seed_result in new_result['individual_results'].items():
                    existing_template['individual_results'][seed] = seed_result

                # Recalculate summary statistics based on all seeds
                all_seeds = existing_template['individual_results']
                total_seeds = len(all_seeds)
                seeds_passed = 0
                seeds_failed = 0
                first_failure_seed = None
                first_failure_reason = None
                consecutive_passes = 0

                # Sort seeds numerically for consistent processing
                sorted_seeds = sorted(all_seeds.keys(), key=lambda x: int(x))

                for seed in sorted_seeds:
                    seed_result = all_seeds[seed]
                    # Check if seed passed (same logic as in test_template_seed_range)
                    if 'spoiler_test' in seed_result:
                        passed = seed_result.get('spoiler_test', {}).get('pass_fail') == 'passed'
                    else:
                        passed = seed_result.get('generation', {}).get('success', False)

                    if passed:
                        seeds_passed += 1
                        if first_failure_seed is None:
                            consecutive_passes += 1
                    else:
                        seeds_failed += 1
                        if first_failure_seed is None:
                            first_failure_seed = int(seed)
                            # Get failure reason from the seed result
                            if 'spoiler_test' in seed_result:
                                spoiler_result = seed_result.get('spoiler_test', {})
                                if spoiler_result.get('first_error_line'):
                                    first_failure_reason = f"Test error: {spoiler_result['first_error_line']}"
                                else:
                                    first_failure_reason = f"Test failed at sphere {spoiler_result.get('sphere_reached', 0)}"
                            else:
                                gen_result = seed_result.get('generation', {})
                                if gen_result.get('first_error_line'):
                                    first_failure_reason = f"Generation error: {gen_result['first_error_line']}"
                                else:
                                    first_failure_reason = f"Generation failed with return code {gen_result.get('return_code')}"

                # Update summary fields
                existing_template['total_seeds_tested'] = total_seeds
                existing_template['seeds_passed'] = seeds_passed
                existing_template['seeds_failed'] = seeds_failed
                existing_template['first_failure_seed'] = first_failure_seed
                existing_template['first_failure_reason'] = first_failure_reason
                existing_template['consecutive_passes_before_failure'] = consecutive_passes

                # Update seed_range to reflect actual range
                if sorted_seeds:
                    existing_template['seed_range'] = f"{sorted_seeds[0]}-{sorted_seeds[-1]}" if len(sorted_seeds) > 1 else str(sorted_seeds[0])

                # Update summary stats
                if total_seeds > 0:
                    existing_template['summary']['failure_rate'] = seeds_failed / total_seeds
                    existing_template['summary']['all_passed'] = seeds_failed == 0
                    existing_template['summary']['any_failed'] = seeds_failed > 0

                # Update timestamp
                existing_template['timestamp'] = new_result['timestamp']

            else:
                # No existing seed range results for this template, use new results entirely
                merged['results'][template_name] = new_result
        else:
            # New result is a single seed result
            if existing_has_seed_range:
                # Existing result is a seed range - merge this single seed into it
                existing_template = merged['results'][template_name]

                # Get the seed from the new result
                seed = new_result.get('seed', '1')

                # Add this single seed result to the individual_results
                if 'individual_results' not in existing_template:
                    existing_template['individual_results'] = {}

                existing_template['individual_results'][seed] = new_result

                # Recalculate summary statistics (same logic as above)
                all_seeds = existing_template['individual_results']
                total_seeds = len(all_seeds)
                seeds_passed = 0
                seeds_failed = 0
                first_failure_seed = None
                first_failure_reason = None
                consecutive_passes = 0

                # Sort seeds numerically for consistent processing
                sorted_seeds = sorted(all_seeds.keys(), key=lambda x: int(x))

                for seed in sorted_seeds:
                    seed_result = all_seeds[seed]
                    # Check if seed passed
                    if 'spoiler_test' in seed_result:
                        passed = seed_result.get('spoiler_test', {}).get('pass_fail') == 'passed'
                    else:
                        passed = seed_result.get('generation', {}).get('success', False)

                    if passed:
                        seeds_passed += 1
                        if first_failure_seed is None:
                            consecutive_passes += 1
                    else:
                        seeds_failed += 1
                        if first_failure_seed is None:
                            first_failure_seed = int(seed)
                            # Get failure reason from the seed result
                            if 'spoiler_test' in seed_result:
                                spoiler_result = seed_result.get('spoiler_test', {})
                                if spoiler_result.get('first_error_line'):
                                    first_failure_reason = f"Test error: {spoiler_result['first_error_line']}"
                                else:
                                    first_failure_reason = f"Test failed at sphere {spoiler_result.get('sphere_reached', 0)}"
                            else:
                                gen_result = seed_result.get('generation', {})
                                if gen_result.get('first_error_line'):
                                    first_failure_reason = f"Generation error: {gen_result['first_error_line']}"
                                else:
                                    first_failure_reason = f"Generation failed with return code {gen_result.get('return_code')}"

                # Update summary fields
                existing_template['total_seeds_tested'] = total_seeds
                existing_template['seeds_passed'] = seeds_passed
                existing_template['seeds_failed'] = seeds_failed
                existing_template['first_failure_seed'] = first_failure_seed
                existing_template['first_failure_reason'] = first_failure_reason
                existing_template['consecutive_passes_before_failure'] = consecutive_passes

                # Update seed_range to reflect actual range
                if sorted_seeds:
                    existing_template['seed_range'] = f"{sorted_seeds[0]}-{sorted_seeds[-1]}" if len(sorted_seeds) > 1 else str(sorted_seeds[0])

                # Update summary stats
                if total_seeds > 0:
                    existing_template['summary']['failure_rate'] = seeds_failed / total_seeds
                    existing_template['summary']['all_passed'] = seeds_failed == 0
                    existing_template['summary']['any_failed'] = seeds_failed > 0

                # Update timestamp
                existing_template['timestamp'] = new_result['timestamp']
            else:
                # Single seed result and no existing seed range - replace entirely
                merged['results'][template_name] = new_result

    return merged


def save_results(results: Dict, results_file: str, batch_processing_time: float = None, timestamped_file: str = None):
    """Save results to JSON file and optionally to a timestamped file."""
    results['metadata']['last_updated'] = datetime.now().isoformat()

    # Add batch processing time if provided
    if batch_processing_time is not None:
        results['metadata']['batch_processing_time_seconds'] = round(batch_processing_time, 1)
        results['metadata']['batch_processing_time_minutes'] = round(batch_processing_time / 60, 1)

        # Calculate average time per template if we have results
        if 'results' in results and len(results['results']) > 0:
            avg_time = batch_processing_time / len(results['results'])
            results['metadata']['average_time_per_template_seconds'] = round(avg_time, 1)

    # Save to timestamped file if provided
    if timestamped_file:
        try:
            with open(timestamped_file, 'w') as f:
                json.dump(results, f, indent=2, sort_keys=True)
            print(f"Timestamped results saved to: {timestamped_file}")
        except IOError as e:
            print(f"Error saving timestamped results: {e}")

    # Save to main results file
    try:
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, sort_keys=True)
        print(f"Results saved to: {results_file}")
    except IOError as e:
        print(f"Error saving results: {e}")


def test_template_single_seed(template_file: str, templates_dir: str, project_root: str, world_mapping: Dict[str, Dict], seed: str = "1", export_only: bool = False, spoiler_only: bool = False) -> Dict:
    """Test a single template file and return results."""
    template_name = os.path.basename(template_file)
    game_name = normalize_game_name(template_name)
    
    # Compute seed ID directly from seed number
    try:
        seed_id = compute_seed_id(int(seed))
    except (ValueError, TypeError):
        print(f"Error: Seed '{seed}' is not a valid number")
        seed_id = None
    
    # Get world info using the provided world mapping
    world_info = get_world_info(template_file, templates_dir, world_mapping)
    
    print(f"\n=== Testing {template_name} ===")
    
    result = {
        'template_name': template_name,
        'game_name': game_name,
        'seed': seed,
        'seed_id': seed_id,
        'timestamp': datetime.now().isoformat(),
        'world_info': world_info,
        'generation': {
            'success': False,
            'error_count': 0,
            'warning_count': 0,
            'first_error_line': None,
            'first_warning_line': None,
            'return_code': None,
            'processing_time_seconds': 0
        },
        'spoiler_test': {
            'success': False,
            'pass_fail': 'unknown',
            'sphere_reached': 0,
            'total_spheres': 0,
            'error_count': 0,
            'warning_count': 0,
            'first_error_line': None,
            'first_warning_line': None,
            'return_code': None,
            'processing_time_seconds': 0
        },
        'rules_file': {
            'path': None,
            'size_bytes': 0,
            'size_mb': 0.0
        },
        'analysis': {
            'success': False,
            'error_count': 0,
            'warning_count': 0,
            'first_error_line': None,
            'first_warning_line': None
        }
    }
    
    # Step 1: Run Generate.py (skip if spoiler_only mode)
    if not spoiler_only:
        print(f"Running Generate.py for {template_name}...")
        # Ensure template name has .yaml extension for the file path
        template_file = template_name if template_name.endswith(('.yaml', '.yml')) else f"{template_name}.yaml"
        template_path = f"Templates/{template_file}"
        generate_cmd = [
            "python", "Generate.py", 
            "--weights_file_path", template_path,
            "--multi", "1",
            "--seed", seed
        ]
        
        # Time the generation process
        gen_start_time = time.time()
        gen_return_code, gen_stdout, gen_stderr = run_command(generate_cmd, cwd=project_root, timeout=600)
        gen_end_time = time.time()
        gen_processing_time = round(gen_end_time - gen_start_time, 2)
        
        # Write generate output to file
        generate_output_file = os.path.join(project_root, "generate_output.txt")
        with open(generate_output_file, 'w') as f:
            f.write(f"STDOUT:\n{gen_stdout}\n\nSTDERR:\n{gen_stderr}\n")
        
        # Seed ID is already computed, no need to extract or verify
        
        # Analyze generation output
        full_output = gen_stdout + "\n" + gen_stderr
        gen_error_count, gen_warning_count, gen_first_error, gen_first_warning = count_errors_and_warnings(full_output)
        
        result['generation'].update({
            'success': gen_return_code == 0,
            'return_code': gen_return_code,
            'error_count': gen_error_count,
            'warning_count': gen_warning_count,
            'first_error_line': gen_first_error,
            'first_warning_line': gen_first_warning,
            'processing_time_seconds': gen_processing_time
        })
        
        if gen_return_code != 0:
            print(f"Generation failed with return code {gen_return_code}")
            return result
    else:
        print(f"Skipping generation for {template_name} (spoiler-only mode)")
        result['generation'].update({
            'success': True,  # Assume success since we're skipping
            'return_code': 0,
            'processing_time_seconds': 0,
            'note': 'Skipped in spoiler-only mode'
        })
    
    # Return early if export_only mode
    if export_only:
        print(f"Export completed for {template_name} (export-only mode)")
        return result
    
    # Step 2: Run spoiler test
    print("Running spoiler test...")
    rules_path = f"./presets/{game_name}/{seed_id}/{seed_id}_rules.json"
    
    # Check if rules file exists (files are actually in frontend/presets/)
    full_rules_path = os.path.join(project_root, 'frontend', rules_path.lstrip('./'))
    if not os.path.exists(full_rules_path):
        print(f"Rules file not found: {full_rules_path}")
        result['spoiler_test']['error_count'] = 1
        result['spoiler_test']['first_error_line'] = f"Rules file not found: {rules_path}"
        return result
    
    spoiler_cmd = ["npm", "test", "--mode=test-spoilers", f"--game={game_name}", f"--seed={seed}"]
    spoiler_env = os.environ.copy()
    
    # Time the spoiler test process
    spoiler_start_time = time.time()
    spoiler_return_code, spoiler_stdout, spoiler_stderr = run_command(
        spoiler_cmd, cwd=project_root, timeout=900, env=spoiler_env
    )
    spoiler_end_time = time.time()
    spoiler_processing_time = round(spoiler_end_time - spoiler_start_time, 2)
    
    result['spoiler_test']['return_code'] = spoiler_return_code
    result['spoiler_test']['success'] = spoiler_return_code == 0
    result['spoiler_test']['processing_time_seconds'] = spoiler_processing_time
    
    # Step 3: Run test analysis
    print("Running test analysis...")
    analysis_cmd = ["npm", "run", "test:analyze"]
    analysis_return_code, analysis_stdout, analysis_stderr = run_command(
        analysis_cmd, cwd=project_root, timeout=60
    )
    
    # Read playwright-analysis.txt if it exists
    analysis_file = os.path.join(project_root, "playwright-analysis.txt")
    if os.path.exists(analysis_file):
        try:
            with open(analysis_file, 'r') as f:
                analysis_text = f.read()
            
            # Parse the analysis
            analysis_result = parse_playwright_analysis(analysis_text)
            result['spoiler_test'].update(analysis_result)
            result['analysis']['success'] = True
            
        except IOError:
            result['analysis']['first_error_line'] = "Could not read playwright-analysis.txt"
    else:
        result['analysis']['first_error_line'] = "playwright-analysis.txt not found"
    
    # Read total spheres from spheres_log.jsonl file
    spheres_log_path = os.path.join(project_root, 'frontend', 'presets', game_name, seed_id, f'{seed_id}_spheres_log.jsonl')
    total_spheres = count_total_spheres(spheres_log_path)
    result['spoiler_test']['total_spheres'] = total_spheres
    
    # If test passed, sphere_reached should equal total_spheres
    if result['spoiler_test']['pass_fail'] == 'passed':
        result['spoiler_test']['sphere_reached'] = total_spheres
    
    # Get rules file size
    rules_file_path = os.path.join(project_root, 'frontend', 'presets', game_name, seed_id, f'{seed_id}_rules.json')
    try:
        if os.path.exists(rules_file_path):
            file_size_bytes = os.path.getsize(rules_file_path)
            file_size_mb = round(file_size_bytes / (1024 * 1024), 2)
            result['rules_file'] = {
                'path': f'frontend/presets/{game_name}/{seed_id}/{seed_id}_rules.json',
                'size_bytes': file_size_bytes,
                'size_mb': file_size_mb
            }
        else:
            result['rules_file'] = {
                'path': f'frontend/presets/{game_name}/{seed_id}/{seed_id}_rules.json',
                'size_bytes': 0,
                'size_mb': 0.0,
                'note': 'File not found'
            }
    except OSError:
        result['rules_file'] = {
            'path': f'frontend/presets/{game_name}/{seed_id}/{seed_id}_rules.json',
            'size_bytes': 0,
            'size_mb': 0.0,
            'note': 'Error reading file size'
        }
    
    print(f"Completed {template_name}: Generation={'[PASS]' if result['generation']['success'] else '[FAIL]'}, "
          f"Test={'[PASS]' if result['spoiler_test']['pass_fail'] == 'passed' else '[FAIL]'}, "
          f"Gen Errors={result['generation']['error_count']}, "
          f"Sphere Reached={result['spoiler_test']['sphere_reached']}, "
          f"Max Spheres={result['spoiler_test']['total_spheres']}")
    
    return result


def test_template_seed_range(template_file: str, templates_dir: str, project_root: str, world_mapping: Dict[str, Dict], seed_list: List[int], export_only: bool = False, spoiler_only: bool = False, stop_on_failure: bool = False) -> Dict:
    """Test a template file with multiple seeds and return aggregated results."""
    template_name = os.path.basename(template_file)
    
    print(f"\n=== Testing {template_name} with {len(seed_list)} seeds ===")
    
    # Initialize seed range result
    seed_range_result = {
        'template_name': template_name,
        'seed_range': f"{seed_list[0]}-{seed_list[-1]}" if len(seed_list) > 1 else str(seed_list[0]),
        'total_seeds_tested': 0,
        'seeds_passed': 0,
        'seeds_failed': 0,
        'first_failure_seed': None,
        'first_failure_reason': None,
        'consecutive_passes_before_failure': 0,
        'stop_on_failure': stop_on_failure,
        'timestamp': datetime.now().isoformat(),
        'individual_results': {},
        'summary': {
            'all_passed': False,
            'any_failed': False,
            'failure_rate': 0.0
        }
    }
    
    consecutive_passes = 0
    
    for i, seed in enumerate(seed_list, 1):
        print(f"\n--- Seed {seed} ({i}/{len(seed_list)}) ---")
        
        try:
            # Test this specific seed
            result = test_template_single_seed(
                template_file, templates_dir, project_root, world_mapping, 
                str(seed), export_only, spoiler_only
            )
            
            seed_range_result['individual_results'][str(seed)] = result
            seed_range_result['total_seeds_tested'] += 1
            
            # Check if this seed passed
            if export_only:
                passed = result.get('generation', {}).get('success', False)
            else:
                passed = result.get('spoiler_test', {}).get('pass_fail') == 'passed'
            
            if passed:
                seed_range_result['seeds_passed'] += 1
                consecutive_passes += 1
                print(f"âœ… Seed {seed} PASSED")
            else:
                seed_range_result['seeds_failed'] += 1
                print(f"âŒ Seed {seed} FAILED")
                
                # Record first failure
                if seed_range_result['first_failure_seed'] is None:
                    seed_range_result['first_failure_seed'] = seed
                    seed_range_result['consecutive_passes_before_failure'] = consecutive_passes
                    
                    # Determine failure reason
                    if export_only:
                        gen_result = result.get('generation', {})
                        if gen_result.get('first_error_line'):
                            seed_range_result['first_failure_reason'] = f"Generation error: {gen_result['first_error_line']}"
                        else:
                            seed_range_result['first_failure_reason'] = f"Generation failed with return code {gen_result.get('return_code')}"
                    else:
                        spoiler_result = result.get('spoiler_test', {})
                        if spoiler_result.get('first_error_line'):
                            seed_range_result['first_failure_reason'] = f"Test error: {spoiler_result['first_error_line']}"
                        else:
                            seed_range_result['first_failure_reason'] = f"Test failed at sphere {spoiler_result.get('sphere_reached', 0)}"
                
                # Stop on failure if requested
                if stop_on_failure:
                    print(f"Stopping at first failure (seed {seed})")
                    break
                    
                # Reset consecutive passes counter after failure
                consecutive_passes = 0
        
        except Exception as e:
            print(f"âŒ Seed {seed} ERROR: {e}")
            seed_range_result['total_seeds_tested'] += 1
            seed_range_result['seeds_failed'] += 1
            
            # Record as first failure if none yet
            if seed_range_result['first_failure_seed'] is None:
                seed_range_result['first_failure_seed'] = seed
                seed_range_result['consecutive_passes_before_failure'] = consecutive_passes
                seed_range_result['first_failure_reason'] = f"Exception: {str(e)}"
            
            if stop_on_failure:
                print(f"Stopping due to exception on seed {seed}")
                break
            
            consecutive_passes = 0
    
    # Calculate summary statistics
    total_tested = seed_range_result['total_seeds_tested']
    if total_tested > 0:
        seed_range_result['summary']['failure_rate'] = seed_range_result['seeds_failed'] / total_tested
        seed_range_result['summary']['all_passed'] = seed_range_result['seeds_failed'] == 0
        seed_range_result['summary']['any_failed'] = seed_range_result['seeds_failed'] > 0
    
    # If no failures and we tested all seeds, consecutive passes = total
    if seed_range_result['first_failure_seed'] is None:
        seed_range_result['consecutive_passes_before_failure'] = seed_range_result['seeds_passed']
    
    # Print summary
    print(f"\n=== Seed Range Summary for {template_name} ===")
    print(f"Seeds tested: {total_tested}")
    print(f"Passed: {seed_range_result['seeds_passed']}")
    print(f"Failed: {seed_range_result['seeds_failed']}")
    if seed_range_result['first_failure_seed'] is not None:
        print(f"First failure at seed: {seed_range_result['first_failure_seed']}")
        print(f"Consecutive passes before failure: {seed_range_result['consecutive_passes_before_failure']}")
        print(f"Failure reason: {seed_range_result['first_failure_reason']}")
    else:
        print(f"ðŸŽ‰ All {seed_range_result['seeds_passed']} seeds passed!")
    
    return seed_range_result


def main():
    parser = argparse.ArgumentParser(description='Test all Archipelago template files')
    parser.add_argument(
        '--templates-dir', 
        type=str, 
        help='Path to alternate template directory (default: Players/Templates)'
    )
    parser.add_argument(
        '--output-file',
        type=str,
        default='scripts/output/template-test-results.json',
        help='Output file path (default: scripts/output/template-test-results.json)'
    )
    parser.add_argument(
        '--skip-list',
        type=str,
        nargs='*',
        default=['Archipelago.yaml', 'Universal Tracker.yaml', 'Final Fantasy.yaml', 'Sudoku.yaml'],
        help='List of template files to skip (default: Archipelago.yaml Universal Tracker.yaml Final Fantasy.yaml Sudoku.yaml)'
    )
    parser.add_argument(
        '--include-list',
        type=str,
        nargs='*',
        help='List of template files to test (if specified, only these files will be tested, overrides skip-list)'
    )
    parser.add_argument(
        '--export-only',
        action='store_true',
        help='Only run the generation (export) step, skip spoiler tests'
    )
    parser.add_argument(
        '--spoiler-only',
        action='store_true',
        help='Only run the spoiler test step, skip generation (requires existing rules files)'
    )
    parser.add_argument(
        '--start-from',
        type=str,
        help='Start processing from the specified template file (alphabetically ordered), skipping all files before it'
    )
    parser.add_argument(
        '-s', '--seed',
        type=str,
        default=None,
        help='Seed number to use for generation (default: 1)'
    )
    parser.add_argument(
        '--seed-range',
        type=str,
        help='Test a range of seeds (format: start-end, e.g., "1-10"). Reports how many seeds passed before first failure.'
    )
    parser.add_argument(
        '--seed-range-continue-on-failure',
        action='store_true',
        help='When testing seed ranges, continue testing all seeds even after failures (default is to stop at first failure)'
    )
    parser.add_argument(
        '-p', '--post-process',
        action='store_true',
        help='Run post-processing scripts after testing (generate-test-chart.py and update-preset-files.py)'
    )
    
    args = parser.parse_args()
    
    # Validate mutually exclusive options
    if args.export_only and args.spoiler_only:
        print("Error: --export-only and --spoiler-only are mutually exclusive")
        sys.exit(1)
    
    # Check if both seed and seed_range were explicitly provided
    if args.seed is not None and args.seed_range is not None:
        print("Error: --seed and --seed-range are mutually exclusive")
        sys.exit(1)
    
    # Parse seed range if provided
    seed_list = []
    if args.seed_range:
        try:
            if '-' in args.seed_range:
                start_seed, end_seed = args.seed_range.split('-', 1)
                start_seed = int(start_seed.strip())
                end_seed = int(end_seed.strip())
                if start_seed > end_seed:
                    print(f"Error: Invalid seed range {start_seed}-{end_seed} (start > end)")
                    sys.exit(1)
                seed_list = list(range(start_seed, end_seed + 1))
            else:
                seed_list = [int(args.seed_range)]
        except ValueError:
            print(f"Error: Invalid seed range format '{args.seed_range}'. Use format like '1-10' or single number.")
            sys.exit(1)
    else:
        # Use the provided seed or default to 1
        seed_to_use = args.seed if args.seed is not None else '1'
        try:
            seed_list = [int(seed_to_use)]
        except ValueError:
            print(f"Error: Invalid seed '{seed_to_use}'. Must be a number.")
            sys.exit(1)
    
    # Check virtual environment before proceeding
    venv_active = 'VIRTUAL_ENV' in os.environ
    deps_available = check_virtual_environment()
    
    if not deps_available:
        print("[ERROR] Required dependencies not available!")
        print("")
        print("Please activate your virtual environment first:")
        print("  Linux/Mac: source .venv/bin/activate")
        print("  Windows:   .venv\\Scripts\\activate")
        print("")
        print("If you haven't set up the development environment, please follow")
        print("the getting-started guide first.")
        sys.exit(1)
    elif not venv_active:
        print("[WARNING] Virtual environment not detected, but dependencies are available.")
        print("   For best results, activate your virtual environment:")
        print("   Linux/Mac: source .venv/bin/activate")
        print("   Windows:   .venv\\Scripts\\activate")
        print("")
        print("Continuing anyway...")
        print("")
    
    # Check if HTTP server is running (required for spoiler tests, but not for export-only)
    if not args.export_only and not check_http_server():
        print("[ERROR] HTTP development server not running!")
        print("")
        print("The spoiler tests require a local development server.")
        print("Please start the server first:")
        print("  python -m http.server 8000")
        print("")
        print("Then access the frontend at: http://localhost:8000/frontend/")
        print("Once the server is running, run this script again.")
        print("")
        print("Alternatively, use --export-only to skip spoiler tests.")
        sys.exit(1)
    
    # Determine project root and templates directory
    project_root = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))
    
    if args.templates_dir:
        templates_dir = os.path.abspath(args.templates_dir)
    else:
        templates_dir = os.path.join(project_root, 'Players', 'Templates')
    
    if not os.path.exists(templates_dir):
        print(f"Error: Templates directory not found: {templates_dir}")
        sys.exit(1)
    
    # Get list of YAML files
    all_yaml_files = [f for f in os.listdir(templates_dir) if f.endswith('.yaml')]
    if not all_yaml_files:
        print(f"Error: No YAML files found in {templates_dir}")
        sys.exit(1)
    
    # Handle include list vs skip list logic
    if args.include_list is not None:
        # Include list mode: only test specified files
        # Allow matching with or without .yaml extension
        yaml_files = []
        for requested_file in args.include_list:
            # Try exact match first
            if requested_file in all_yaml_files:
                yaml_files.append(requested_file)
            # Try adding .yaml extension
            elif not requested_file.endswith('.yaml') and f"{requested_file}.yaml" in all_yaml_files:
                yaml_files.append(f"{requested_file}.yaml")
            # Try removing .yaml extension and finding match
            elif requested_file.endswith('.yaml'):
                base_name = requested_file[:-5]
                matching_file = next((f for f in all_yaml_files if f.startswith(base_name)), None)
                if matching_file:
                    yaml_files.append(matching_file)
        
        # Remove duplicates while preserving order
        yaml_files = list(dict.fromkeys(yaml_files))
        skipped_files = [f for f in all_yaml_files if f not in yaml_files]
        
        # Check if any requested files don't exist
        found_files = set(yaml_files)
        missing_files = []
        for requested in args.include_list:
            if requested not in found_files and f"{requested}.yaml" not in found_files:
                # Check if we found it by removing .yaml
                base_name = requested[:-5] if requested.endswith('.yaml') else requested
                if not any(f.startswith(base_name) for f in found_files):
                    missing_files.append(requested)
        
        if missing_files:
            print(f"Warning: Requested files not found: {', '.join(missing_files)}")
        
        if not yaml_files:
            print(f"Error: None of the requested files were found in {templates_dir}")
            if args.include_list:
                print(f"Requested: {', '.join(args.include_list)}")
                print(f"Available: {', '.join(all_yaml_files)}")
            sys.exit(1)
        
        filter_description = f"include list ({len(args.include_list)} requested)"
    else:
        # Skip list mode: exclude specified files
        yaml_files = [f for f in all_yaml_files if f not in args.skip_list]
        skipped_files = [f for f in all_yaml_files if f in args.skip_list]
        
        if not yaml_files:
            print(f"Error: No testable YAML files found after filtering (all files are in skip list)")
            sys.exit(1)
        
        filter_description = f"skip list ({len(args.skip_list)} excluded)"
    
    yaml_files.sort()
    
    # Handle --start-from option
    if args.start_from:
        if args.start_from not in yaml_files:
            print(f"Error: Start file '{args.start_from}' not found in testable files")
            print(f"Available testable files: {', '.join(yaml_files)}")
            sys.exit(1)
        
        # Find the index and slice from there
        start_index = yaml_files.index(args.start_from)
        before_start = yaml_files[:start_index]
        yaml_files = yaml_files[start_index:]
        
        print(f"Found {len(all_yaml_files)} template files ({len(yaml_files)} will be tested, {len(skipped_files) + len(before_start)} filtered)")
        print(f"Starting from: {args.start_from} (skipping {len(before_start)} files before it)")
    else:
        print(f"Found {len(all_yaml_files)} template files ({len(yaml_files)} testable, {len(skipped_files)} filtered by {filter_description})")
    
    if skipped_files and len(skipped_files) <= 10:  # Only show if reasonable number
        if args.include_list is not None:
            print(f"Not included: {', '.join(skipped_files)}")
        else:
            print(f"Skipping: {', '.join(skipped_files)}")
    elif len(skipped_files) > 10:
        print(f"{'Not included' if args.include_list is not None else 'Skipping'}: {len(skipped_files)} files (too many to list)")
    
    # Load existing results for merging
    results_file = os.path.join(project_root, args.output_file)
    existing_results = load_existing_results(results_file)

    # Determine if we should update metadata in merged results
    # Only update metadata for full runs (no --include-list)
    update_metadata = args.include_list is None

    # Create new results structure for this run
    results = {
        'metadata': {
            'created': datetime.now().isoformat(),
            'last_updated': datetime.now().isoformat(),
            'script_version': '1.0.0'
        },
        'results': {}
    }

    # Ensure output directory exists
    os.makedirs(os.path.dirname(results_file), exist_ok=True)

    # Generate timestamped filename
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    output_dir = os.path.dirname(results_file)
    output_basename = os.path.basename(results_file)
    # Insert timestamp before file extension
    name_parts = output_basename.rsplit('.', 1)
    if len(name_parts) == 2:
        timestamped_basename = f"{name_parts[0]}_{timestamp}.{name_parts[1]}"
    else:
        timestamped_basename = f"{output_basename}_{timestamp}"
    timestamped_file = os.path.join(output_dir, timestamped_basename)

    print(f"Results will be saved to: {results_file}")
    print(f"Timestamped backup will be saved to: {timestamped_basename}")
    print(f"Testing templates from: {templates_dir}")
    
    # Build world mapping once at startup
    world_mapping = build_and_load_world_mapping(project_root)
    
    # Display seed information
    if len(seed_list) == 1:
        computed_seed_id = compute_seed_id(seed_list[0])
        print(f"Using seed {seed_list[0]} -> {computed_seed_id}")
    else:
        print(f"Testing seed range: {seed_list[0]}-{seed_list[-1]} ({len(seed_list)} seeds)")
        if args.seed_range_continue_on_failure:
            print("Will test all seeds regardless of failures")
        else:
            print("Will stop at first failure (default behavior)")
    
    # Start timing the batch processing
    batch_start_time = time.time()
    print(f"Starting batch processing of {len(yaml_files)} templates...")
    
    # Test each template
    total_files = len(yaml_files)
    for i, yaml_file in enumerate(yaml_files, 1):
        print(f"\n[{i}/{total_files}] Processing {yaml_file}")
        
        try:
            if len(seed_list) > 1:
                # Test with seed range
                template_result = test_template_seed_range(
                    yaml_file, templates_dir, project_root, world_mapping, 
                    seed_list, export_only=args.export_only, spoiler_only=args.spoiler_only,
                    stop_on_failure=not args.seed_range_continue_on_failure
                )
            else:
                # Test with single seed
                template_result = test_template_single_seed(
                    yaml_file, templates_dir, project_root, world_mapping, 
                    str(seed_list[0]), export_only=args.export_only, spoiler_only=args.spoiler_only
                )
            
            results['results'][yaml_file] = template_result

            # Save results after each template (incremental updates)
            # Merge with existing results and save
            templates_tested_so_far = list(results['results'].keys())
            incremental_merged = merge_results(existing_results, results, templates_tested_so_far, update_metadata)
            save_results(incremental_merged, results_file)

            # Run post-processing after each test if requested
            if args.post_process:
                run_post_processing_scripts(project_root, results_file)

        except KeyboardInterrupt:
            print("\nInterrupted by user. Saving current results...")
            templates_tested_so_far = list(results['results'].keys())
            incremental_merged = merge_results(existing_results, results, templates_tested_so_far, update_metadata)
            save_results(incremental_merged, results_file)
            # Also save timestamped partial results
            save_results(results, results_file, timestamped_file=timestamped_file)
            sys.exit(1)
        except Exception as e:
            print(f"Error processing {yaml_file}: {e}")
            # Create minimal error result
            error_result = {
                'template_name': yaml_file,
                'timestamp': datetime.now().isoformat(),
                'error': str(e)
            }
            results['results'][yaml_file] = error_result
            # Save results after error (incremental updates)
            templates_tested_so_far = list(results['results'].keys())
            incremental_merged = merge_results(existing_results, results, templates_tested_so_far, update_metadata)
            save_results(incremental_merged, results_file)
    
    # Calculate total batch processing time
    batch_end_time = time.time()
    total_batch_time = batch_end_time - batch_start_time

    # Save timestamped results file (snapshot of this run only)
    save_results(results, results_file, total_batch_time, timestamped_file)

    # Merge this run's results into the existing results
    merged_results = merge_results(existing_results, results, yaml_files, update_metadata)

    # Save merged results to main file
    save_results(merged_results, results_file)

    print(f"\n=== Testing Complete ===")
    print(f"Processed {len(yaml_files)} templates")
    print(f"Total batch processing time: {total_batch_time:.1f} seconds ({total_batch_time/60:.1f} minutes)")
    if len(yaml_files) > 0:
        avg_time_per_template = total_batch_time / len(yaml_files)
        print(f"Average time per template: {avg_time_per_template:.1f} seconds")
    print(f"Timestamped results saved to: {timestamped_file}")
    print(f"Merged results saved to: {results_file}")
    
    # Print summary based on mode and seed range
    print(f"\n=== Final Summary ===")
    
    if len(seed_list) > 1:
        # Seed range testing summary
        templates_with_all_seeds_passed = 0
        templates_with_some_failures = 0
        total_consecutive_passes = 0
        
        print(f"Seed Range Testing Results ({len(seed_list)} seeds per template):")
        
        for template_name, result in results['results'].items():
            if 'seed_range' in result:
                seeds_passed = result.get('seeds_passed', 0)
                seeds_failed = result.get('seeds_failed', 0)
                consecutive_passes = result.get('consecutive_passes_before_failure', 0)
                first_failure_seed = result.get('first_failure_seed')
                
                total_consecutive_passes += consecutive_passes
                
                if seeds_failed == 0:
                    templates_with_all_seeds_passed += 1
                    print(f"  âœ… {template_name}: All {seeds_passed} seeds passed")
                else:
                    templates_with_some_failures += 1
                    if first_failure_seed:
                        print(f"  âŒ {template_name}: {consecutive_passes} consecutive passes, first failure at seed {first_failure_seed}")
                    else:
                        print(f"  âŒ {template_name}: {seeds_passed} passed, {seeds_failed} failed")
        
        print(f"\nOverall Seed Range Summary:")
        print(f"  Templates with all seeds passing: {templates_with_all_seeds_passed}")
        print(f"  Templates with some failures: {templates_with_some_failures}")
        if len(yaml_files) > 0:
            avg_consecutive = total_consecutive_passes / len(yaml_files)
            print(f"  Average consecutive passes before failure: {avg_consecutive:.1f}")
    
    else:
        # Single seed testing summary
        if args.export_only:
            successful_exports = sum(1 for r in results['results'].values() 
                                   if r.get('generation', {}).get('success', False))
            failed_exports = len(yaml_files) - successful_exports
            print(f"Export Summary: {successful_exports} successful, {failed_exports} failed")
        elif args.spoiler_only:
            passed = sum(1 for r in results['results'].values() 
                        if r.get('spoiler_test', {}).get('pass_fail') == 'passed')
            failed = sum(1 for r in results['results'].values() 
                        if r.get('spoiler_test', {}).get('pass_fail') == 'failed')
            errors = len(yaml_files) - passed - failed
            print(f"Spoiler Test Summary: {passed} passed, {failed} failed, {errors} errors")
        else:
            passed = sum(1 for r in results['results'].values() 
                        if r.get('spoiler_test', {}).get('pass_fail') == 'passed')
            failed = sum(1 for r in results['results'].values() 
                        if r.get('spoiler_test', {}).get('pass_fail') == 'failed')
            errors = len(yaml_files) - passed - failed
            print(f"Single Seed Test Summary: {passed} passed, {failed} failed, {errors} errors")
    
    # Run post-processing scripts if requested (only if not already run after each test)
    # This ensures post-processing runs at least once, even if no tests were run
    if args.post_process and len(yaml_files) == 0:
        run_post_processing_scripts(project_root, results_file)


if __name__ == '__main__':
    main()